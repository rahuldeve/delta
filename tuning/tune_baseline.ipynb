{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea763b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data, featurizers, models\n",
    "from chemprop import nn as chem_nn\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "import pandas as pd\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem.Descriptors import CalcMolDescriptors\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from commons.utils import get_scaffold, standardize\n",
    "from typing import NamedTuple\n",
    "from itertools import chain\n",
    "from delta_data import RandomPairDataModule\n",
    "import chemprop as cp\n",
    "from delta_model import DeltaProp, Encoder, Interaction\n",
    "from ray import tune, train\n",
    "import ray\n",
    "\n",
    "import wandb\n",
    "# from commons.data import load_and_split_gsk_dataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "# load_dotenv('.env.secret')\n",
    "wandb.login(key='cf344975eb80edf6f0d52af80528cc6094234caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_inchi(mol):\n",
    "    with BlockLogs():\n",
    "        return Chem.MolToInchi(mol)\n",
    "    \n",
    "\n",
    "def generate_features(df):\n",
    "    with BlockLogs():\n",
    "        feats = pd.DataFrame.from_records(df[\"mol\"].map(CalcMolDescriptors).tolist())\n",
    "        feats.columns = [f\"feat_{f}\" for f in feats.columns]\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df.reset_index(drop=True),\n",
    "                feats,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_split_gsk_dataset(path, RANDOM_SEED):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[:, 1:]\n",
    "    df.columns = [\"smiles\", \"per_inhibition\"]\n",
    "\n",
    "    # standardize and convert to inchi\n",
    "    df[\"mol\"] = df[\"smiles\"].map(standardize)\n",
    "    df = df.dropna(subset=[\"mol\"])\n",
    "    df[\"inchi\"] = df[\"mol\"].map(mol_to_inchi)\n",
    "    df = df.groupby([\"inchi\"]).filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "\n",
    "    df[\"is_cytotoxic\"] = df[\"per_inhibition\"] > 50.0\n",
    "\n",
    "    df = generate_features(df)\n",
    "\n",
    "    clusters, _ = pd.factorize(\n",
    "        df[\"mol\"]\n",
    "        .map(Chem.MolToSmiles)  # type: ignore\n",
    "        .map(get_scaffold)\n",
    "    )\n",
    "    clusters = pd.Series(clusters)\n",
    "\n",
    "    df = df.drop([\"smiles\", \"inchi\"], axis=1)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED)\n",
    "    train_idxs, val_test_idxs = next(splitter.split(df, groups=clusters))\n",
    "    df_train = df.loc[train_idxs].reset_index(drop=True)\n",
    "    df_val_test = df.loc[val_test_idxs].reset_index(drop=True)\n",
    "    clusters_val_test = clusters.iloc[val_test_idxs].reset_index(drop=True)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED, test_size=0.5)\n",
    "    val_idxs, test_idxs = next(splitter.split(df_val_test, groups=clusters_val_test))\n",
    "    df_val = df_val_test.loc[val_idxs].reset_index(drop=True)\n",
    "    df_test = df_val_test.loc[test_idxs].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_and_split_gsk_dataset(\"../GSK_HepG2.csv\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08938323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_datapoint(row):\n",
    "    feat_entry_names = [f for f in row.index if f.startswith('feat')]\n",
    "    feat_array = pd.to_numeric(row[feat_entry_names], errors=\"coerce\")\n",
    "    return cp.data.MoleculeDatapoint(\n",
    "        mol=row['mol'],\n",
    "        y=np.array([row['per_inhibition'] > 50]),\n",
    "        x_d=feat_array.to_numpy()\n",
    "    )\n",
    "\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "train_mol_dataset = data.MoleculeDataset(df_train.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "val_mol_dataset = data.MoleculeDataset(df_val.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "test_mol_dataset = data.MoleculeDataset(df_test.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "\n",
    "x_d_scaler = train_mol_dataset.normalize_inputs(\"X_d\")\n",
    "val_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "test_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "\n",
    "train_mol_dataset.cache = True\n",
    "val_mol_dataset.cache = True\n",
    "test_mol_dataset.cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\n",
    "\n",
    "def tune_func(config, train_mol_dataset, val_mol_dataset):\n",
    "    depth = config[\"depth\"]\n",
    "    ffn_hidden_dim = config[\"ffn_hidden_dim\"]\n",
    "    ffn_num_layers = config[\"ffn_num_layers\"]\n",
    "    message_hidden_dim = config[\"message_hidden_dim\"]\n",
    "    batch_norm = config['batch_norm']\n",
    "\n",
    "    train_loader = cp.data.build_dataloader(train_mol_dataset, batch_size=32, num_workers=2, seed=RANDOM_SEED)\n",
    "    val_loader = cp.data.build_dataloader(val_mol_dataset, batch_size=32, num_workers=2, shuffle=False)\n",
    "\n",
    "    ###############################################################################################\n",
    "\n",
    "    mp = cp.nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "    agg = cp.nn.NormAggregation()\n",
    "    ffn_dims = mp.output_dim + train_mol_dataset.X_d.shape[-1]\n",
    "    ffn = cp.nn.BinaryClassificationFFN(n_tasks=1, input_dim=ffn_dims, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
    "    metric_list = [cp.nn.metrics.BinaryF1Score(), cp.nn.metrics.BinaryAUPRC(), cp.nn.metrics.BinaryAUROC()]\n",
    "    X_d_transform = cp.nn.ScaleTransform.from_standard_scaler(x_d_scaler)\n",
    "    mpnn = cp.models.MPNN(mp, agg, ffn, batch_norm, metric_list, X_d_transform=X_d_transform)\n",
    "\n",
    "    ################################################################################################\n",
    "    trainer = L.Trainer(\n",
    "        logger=None,\n",
    "        enable_checkpointing=True,\n",
    "        enable_progress_bar=False,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=20,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=False, patience=10),\n",
    "            TuneReportCheckpointCallback()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9637b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"depth\": tune.qrandint(lower=2, upper=6, q=1),\n",
    "    \"ffn_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"ffn_num_layers\": tune.qrandint(lower=1, upper=3, q=1),\n",
    "    \"message_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"batch_norm\": tune.choice([True, False])\n",
    "}\n",
    "\n",
    "search_alg = ConcurrencyLimiter(OptunaSearch(seed=42), max_concurrent=8)\n",
    "scheduler = ASHAScheduler(max_t=20, grace_period=1, reduction_factor=2)\n",
    "\n",
    "tune_fn = tune.with_resources(\n",
    "    tune.with_parameters(\n",
    "        tune_func, \n",
    "        train_mol_dataset=train_mol_dataset, \n",
    "        val_mol_dataset=val_mol_dataset\n",
    "    ),\n",
    "    resources={\"CPU\": 4, \"GPU\": 0.25}\n",
    ")\n",
    "\n",
    "# Checkpoint config controls the checkpointing behavior of Ray\n",
    "checkpoint_config = tune.CheckpointConfig(\n",
    "    num_to_keep=1, # number of checkpoints to keep\n",
    "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
    "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune_fn,\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=20,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search_alg,\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"val_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "        failure_config = train.FailureConfig(max_failures=3)\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "_, best_result = results.get_best_result().best_checkpoints[0]\n",
    "best_config = best_result['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = best_config[\"depth\"]\n",
    "ffn_hidden_dim = best_config[\"ffn_hidden_dim\"]\n",
    "ffn_num_layers = best_config[\"ffn_num_layers\"]\n",
    "message_hidden_dim = best_config[\"message_hidden_dim\"]\n",
    "batch_norm = best_config['batch_norm']\n",
    "\n",
    "train_loader = cp.data.build_dataloader(train_mol_dataset, batch_size=32, num_workers=1, seed=RANDOM_SEED)\n",
    "val_loader = cp.data.build_dataloader(val_mol_dataset, batch_size=32, num_workers=1, shuffle=False)\n",
    "test_loader = cp.data.build_dataloader(test_mol_dataset, batch_size=32, num_workers=1, shuffle=False)\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "mp = cp.nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "agg = cp.nn.NormAggregation()\n",
    "ffn_dims = mp.output_dim + train_mol_dataset.X_d.shape[-1]\n",
    "ffn = cp.nn.BinaryClassificationFFN(n_tasks=1, input_dim=ffn_dims, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
    "metric_list = [cp.nn.metrics.BinaryF1Score(), cp.nn.metrics.BinaryAUPRC(), cp.nn.metrics.BinaryAUROC()]\n",
    "X_d_transform = cp.nn.ScaleTransform.from_standard_scaler(x_d_scaler)\n",
    "mpnn = cp.models.MPNN(mp, agg, ffn, batch_norm, metric_list, X_d_transform=X_d_transform)\n",
    "\n",
    "################################################################################################\n",
    "trainer = L.Trainer(\n",
    "    logger=None,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=False,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=False, patience=10),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "mpnn = cp.models.MPNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "run = wandb.init(project=\"evaluation\")\n",
    "wandb.mark_preempting()\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    enable_progress_bar=False,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "test_ds_preds = trainer.predict(model=mpnn, dataloaders=test_loader)\n",
    "test_ds_preds = torch.cat(test_ds_preds)\n",
    "\n",
    "pred_probs = test_ds_preds.squeeze().numpy()\n",
    "preds = (pred_probs >= 0.5).astype(float)\n",
    "labels = df_test['per_inhibition'] > 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "run.log({\n",
    "    'final_metrics': wandb.Table(\n",
    "        columns=['accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'AUCROC', 'PRAUC'],\n",
    "        data=[[\n",
    "            accuracy_score(labels, preds),\n",
    "            balanced_accuracy_score(labels, preds),\n",
    "            f1_score(labels, preds),\n",
    "            precision_score(labels, preds),\n",
    "            recall_score(labels, preds),\n",
    "            roc_auc_score(labels, pred_probs),\n",
    "            average_precision_score(labels, pred_probs)\n",
    "        ]]\n",
    "    )\n",
    "})\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6373d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
