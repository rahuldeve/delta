{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea763b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem.Descriptors import CalcMolDescriptors\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmiles\n",
    "import chemprop as cp\n",
    "from delta_model import DeltaProp, Encoder, Interaction\n",
    "from delta_data import RandomPairDataModule\n",
    "from ray import tune, train\n",
    "import ray\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "# load_dotenv('.env.secret')\n",
    "wandb.login(key='cf344975eb80edf6f0d52af80528cc6094234caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a93135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(smiles):\n",
    "    with BlockLogs():\n",
    "        params = Chem.SmilesParserParams()\n",
    "        params.removeHs = False\n",
    "        mol = Chem.MolFromSmiles(smiles, params)  # type: ignore\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        clean_mol = rdMolStandardize.Cleanup(mol)\n",
    "        parent_clean_mol = rdMolStandardize.FragmentParent(clean_mol)\n",
    "        uncharger = rdMolStandardize.Uncharger()\n",
    "        uncharged_parent_clean_mol = uncharger.uncharge(parent_clean_mol)\n",
    "        return pickle.dumps(uncharged_parent_clean_mol)\n",
    "\n",
    "def mol_to_inchi(mol):\n",
    "    with BlockLogs():\n",
    "        return Chem.MolToInchi(mol)\n",
    "\n",
    "def generate_features(mol):\n",
    "    with BlockLogs():\n",
    "        return {f\"feat_{k}\": v for k, v in CalcMolDescriptors(mol).items()}\n",
    "    \n",
    "def get_scaffold(mol) -> str:\n",
    "    smi = Chem.MolToSmiles(mol)\n",
    "    scaffold = MurckoScaffoldSmiles(smi)\n",
    "    if len(scaffold) == 0:\n",
    "        scaffold = smi\n",
    "    return scaffold\n",
    "    \n",
    "\n",
    "df = pd.read_csv(\"../GSK_HepG2.csv\").sample(7000)\n",
    "df = df.iloc[:, 1:]\n",
    "df.columns = [\"smiles\", \"per_inhibition\"]\n",
    "\n",
    "df = (\n",
    "    (\n",
    "        ray.data.from_pandas(df.reset_index(), override_num_blocks=len(df) // 64)\n",
    "        .map(lambda row: row | {\"mol_ser\": standardize(row[\"smiles\"])})\n",
    "        .filter(lambda row: row[\"mol_ser\"] is not None)\n",
    "        .map(lambda row: row | {\"inchi\": mol_to_inchi(pickle.loads(row[\"mol_ser\"]))})\n",
    "        .map(lambda row: row | generate_features(pickle.loads(row[\"mol_ser\"])))\n",
    "        .map(lambda row: row | {\"scaffold\": get_scaffold(pickle.loads(row[\"mol_ser\"]))})\n",
    "    )\n",
    "    .materialize()\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "df = df.groupby([\"inchi\"]).filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "\n",
    "clusters, _ = pd.factorize(df[\"scaffold\"])\n",
    "clusters = pd.Series(clusters)\n",
    "\n",
    "df['mol'] = df['mol_ser'].map(pickle.loads)\n",
    "df = df.drop([\"smiles\", \"inchi\", \"scaffold\", \"mol_ser\"], axis=1)\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED)\n",
    "train_idxs, val_test_idxs = next(splitter.split(df, groups=clusters))\n",
    "df_train = df.loc[train_idxs].reset_index(drop=True)\n",
    "df_val_test = df.loc[val_test_idxs].reset_index(drop=True)\n",
    "clusters_val_test = clusters.iloc[val_test_idxs].reset_index(drop=True)\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED, test_size=0.5)\n",
    "val_idxs, test_idxs = next(splitter.split(df_val_test, groups=clusters_val_test))\n",
    "df_val = df_val_test.loc[val_idxs].reset_index(drop=True)\n",
    "df_test = df_val_test.loc[test_idxs].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08938323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_datapoint(row):\n",
    "    feat_entry_names = [f for f in row.index if f.startswith('feat')]\n",
    "    feat_array = pd.to_numeric(row[feat_entry_names], errors=\"coerce\")\n",
    "    return cp.data.MoleculeDatapoint(\n",
    "        mol=row['mol'], \n",
    "        y=np.array([row['per_inhibition']]),\n",
    "        x_d=feat_array.to_numpy()\n",
    "    )\n",
    "\n",
    "featurizer = cp.featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "train_mol_dataset = cp.data.MoleculeDataset(df_train.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "val_mol_dataset = cp.data.MoleculeDataset(df_val.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "test_mol_dataset = cp.data.MoleculeDataset(df_test.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "\n",
    "x_d_scaler = train_mol_dataset.normalize_inputs(\"X_d\")\n",
    "val_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "test_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "\n",
    "train_mol_dataset.cache = True\n",
    "val_mol_dataset.cache = True\n",
    "test_mol_dataset.cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\n",
    "\n",
    "def tune_func(config, train_mol_dataset, val_mol_dataset):\n",
    "    depth = config[\"depth\"]\n",
    "    ffn_hidden_dim = config[\"ffn_hidden_dim\"]\n",
    "    ffn_num_layers = config[\"ffn_num_layers\"]\n",
    "    message_hidden_dim = config[\"message_hidden_dim\"]\n",
    "    batch_norm = config['batch_norm']\n",
    "    encoder_dropout = config[\"encoder_dropout\"]\n",
    "    interaction_dropout = config[\"interaction_dropout\"]\n",
    "\n",
    "    ###############################################################################################\n",
    "\n",
    "    mp = cp.nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "    agg = cp.nn.NormAggregation()\n",
    "    ffn_dims = mp.output_dim + train_mol_dataset.X_d.shape[-1]\n",
    "    encoder = Encoder(\n",
    "        input_dim=ffn_dims, \n",
    "        hidden_dim=ffn_hidden_dim, \n",
    "        n_layers=ffn_num_layers, \n",
    "        activation=torch.nn.ELU(), \n",
    "        dropout=encoder_dropout\n",
    "    )\n",
    "    interaction = Interaction(encoder.output_dim, dropout=interaction_dropout)\n",
    "    model = DeltaProp(mp, agg, encoder, interaction, batch_norm=batch_norm)\n",
    "\n",
    "    ################################################################################################\n",
    "    trainer = L.Trainer(\n",
    "        logger=None,\n",
    "        enable_checkpointing=True,\n",
    "        enable_progress_bar=False,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=20,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=False, patience=8),\n",
    "            TuneReportCheckpointCallback()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=RandomPairDataModule(train_mol_dataset, val_mol_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9637b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"depth\": tune.qrandint(lower=2, upper=6, q=1),\n",
    "    \"ffn_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"ffn_num_layers\": tune.qrandint(lower=1, upper=3, q=1),\n",
    "    \"message_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"encoder_dropout\": tune.uniform(lower=0.0, upper=0.3),\n",
    "    \"interaction_dropout\": tune.uniform(lower=0.0, upper=0.3),\n",
    "    \"batch_norm\": tune.choice([True, False])\n",
    "}\n",
    "\n",
    "search_alg = ConcurrencyLimiter(OptunaSearch(seed=42), max_concurrent=3)\n",
    "scheduler = ASHAScheduler(max_t=20, grace_period=1, reduction_factor=2)\n",
    "\n",
    "tune_fn = tune.with_resources(\n",
    "    tune.with_parameters(\n",
    "        tune_func, \n",
    "        train_mol_dataset=train_mol_dataset, \n",
    "        val_mol_dataset=val_mol_dataset\n",
    "    ),\n",
    "    resources={\"GPU\": 0.5}\n",
    ")\n",
    "\n",
    "# Checkpoint config controls the checkpointing behavior of Ray\n",
    "checkpoint_config = tune.CheckpointConfig(\n",
    "    num_to_keep=1, # number of checkpoints to keep\n",
    "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
    "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune_fn,\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=20,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search_alg,\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"val_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "        failure_config = train.FailureConfig(max_failures=3)\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "_, best_result = results.get_best_result().best_checkpoints[0]\n",
    "best_config = best_result['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = best_config[\"depth\"]\n",
    "ffn_hidden_dim = best_config[\"ffn_hidden_dim\"]\n",
    "ffn_num_layers = best_config[\"ffn_num_layers\"]\n",
    "message_hidden_dim = best_config[\"message_hidden_dim\"]\n",
    "batch_norm = best_config['batch_norm']\n",
    "encoder_dropout = best_config[\"encoder_dropout\"]\n",
    "interaction_dropout = best_config[\"interaction_dropout\"]\n",
    "\n",
    "train_loader = cp.data.build_dataloader(train_mol_dataset, batch_size=32, num_workers=8, seed=RANDOM_SEED)\n",
    "val_loader = cp.data.build_dataloader(val_mol_dataset, batch_size=32, num_workers=8, shuffle=False)\n",
    "test_loader = cp.data.build_dataloader(test_mol_dataset, batch_size=32, num_workers=8, shuffle=False)\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "mp = cp.nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "agg = cp.nn.NormAggregation()\n",
    "ffn_dims = mp.output_dim + train_mol_dataset.X_d.shape[-1]\n",
    "encoder = Encoder(\n",
    "    input_dim=ffn_dims, \n",
    "    hidden_dim=ffn_hidden_dim, \n",
    "    n_layers=ffn_num_layers, \n",
    "    activation=torch.nn.ELU(), \n",
    "    dropout=encoder_dropout\n",
    ")\n",
    "interaction = Interaction(encoder.output_dim, dropout=interaction_dropout)\n",
    "model = DeltaProp(mp, agg, encoder, interaction, batch_norm=batch_norm)\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"chemprop_delta_clf\", log_model=\"all\", save_code=True)\n",
    "wandb_logger.watch(model, log=\"gradients\", log_freq=50) \n",
    "wandb_logger.experiment.mark_preempting()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=3,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=10),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=RandomPairDataModule(train_mol_dataset, val_mol_dataset))\n",
    "model = DeltaProp.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "run = wandb.init(project=\"evaluation\")\n",
    "wandb.mark_preempting()\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    enable_progress_bar=False,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "test_ds_preds = trainer.predict(model, dataloaders=test_loader)\n",
    "test_ds_preds = torch.cat(test_ds_preds)\n",
    "\n",
    "pred_probs = test_ds_preds.squeeze().numpy()\n",
    "preds = (pred_probs >= 0.5).astype(float)\n",
    "labels = df_test['per_inhibition'] > 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "run.log({\n",
    "    'final_metrics': wandb.Table(\n",
    "        columns=['accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'AUCROC', 'PRAUC'],\n",
    "        data=[[\n",
    "            accuracy_score(labels, preds),\n",
    "            balanced_accuracy_score(labels, preds),\n",
    "            f1_score(labels, preds),\n",
    "            precision_score(labels, preds),\n",
    "            recall_score(labels, preds),\n",
    "            roc_auc_score(labels, pred_probs),\n",
    "            average_precision_score(labels, pred_probs)\n",
    "        ]]\n",
    "    )\n",
    "})\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6373d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
