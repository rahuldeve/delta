{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea763b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul-e-dev\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data, featurizers, models\n",
    "from chemprop import nn as chem_nn\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "import pandas as pd\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem.Descriptors import CalcMolDescriptors\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from commons.utils import get_scaffold, standardize\n",
    "from typing import NamedTuple\n",
    "from itertools import chain\n",
    "from delta_data import RandomPairDataModule\n",
    "import chemprop as cp\n",
    "from delta_model import DeltaProp, Encoder, Interaction\n",
    "from ray import tune, train\n",
    "import ray\n",
    "\n",
    "import wandb\n",
    "# from commons.data import load_and_split_gsk_dataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "# load_dotenv('.env.secret')\n",
    "wandb.login(key='cf344975eb80edf6f0d52af80528cc6094234caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_inchi(mol):\n",
    "    with BlockLogs():\n",
    "        return Chem.MolToInchi(mol)\n",
    "    \n",
    "\n",
    "def generate_features(df):\n",
    "    with BlockLogs():\n",
    "        feats = pd.DataFrame.from_records(df[\"mol\"].map(CalcMolDescriptors).tolist())\n",
    "        feats.columns = [f\"feat_{f}\" for f in feats.columns]\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df.reset_index(drop=True),\n",
    "                feats,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_split_gsk_dataset(path, RANDOM_SEED):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[:, 1:]\n",
    "    df.columns = [\"smiles\", \"per_inhibition\"]\n",
    "\n",
    "    # standardize and convert to inchi\n",
    "    df[\"mol\"] = df[\"smiles\"].map(standardize)\n",
    "    df = df.dropna(subset=[\"mol\"])\n",
    "    df[\"inchi\"] = df[\"mol\"].map(mol_to_inchi)\n",
    "    df = df.groupby([\"inchi\"]).filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "\n",
    "    df[\"is_cytotoxic\"] = df[\"per_inhibition\"] > 50.0\n",
    "\n",
    "    df = generate_features(df)\n",
    "\n",
    "    clusters, _ = pd.factorize(\n",
    "        df[\"mol\"]\n",
    "        .map(Chem.MolToSmiles)  # type: ignore\n",
    "        .map(get_scaffold)\n",
    "    )\n",
    "    clusters = pd.Series(clusters)\n",
    "\n",
    "    df = df.drop([\"smiles\", \"inchi\"], axis=1)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED)\n",
    "    train_idxs, val_test_idxs = next(splitter.split(df, groups=clusters))\n",
    "    df_train = df.loc[train_idxs].reset_index(drop=True)\n",
    "    df_val_test = df.loc[val_test_idxs].reset_index(drop=True)\n",
    "    clusters_val_test = clusters.iloc[val_test_idxs].reset_index(drop=True)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED, test_size=0.5)\n",
    "    val_idxs, test_idxs = next(splitter.split(df_val_test, groups=clusters_val_test))\n",
    "    df_val = df_val_test.loc[val_idxs].reset_index(drop=True)\n",
    "    df_test = df_val_test.loc[test_idxs].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce81216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_and_split_gsk_dataset(\"../GSK_HepG2.csv\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08938323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_datapoint(row):\n",
    "    feat_entry_names = [f for f in row.index if f.startswith('feat')]\n",
    "    feat_array = pd.to_numeric(row[feat_entry_names], errors=\"coerce\")\n",
    "    return cp.data.MoleculeDatapoint(\n",
    "        mol=row['mol'],\n",
    "        y=np.array([row['per_inhibition'] > 50]),\n",
    "        x_d=feat_array.to_numpy()\n",
    "    )\n",
    "\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "train_mol_dataset = data.MoleculeDataset(df_train.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "val_mol_dataset = data.MoleculeDataset(df_val.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "test_mol_dataset = data.MoleculeDataset(df_test.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "\n",
    "x_d_scaler = train_mol_dataset.normalize_inputs(\"X_d\")\n",
    "val_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "test_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "\n",
    "train_mol_dataset.cache = True\n",
    "val_mol_dataset.cache = True\n",
    "test_mol_dataset.cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84cd7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\n",
    "\n",
    "def tune_func(config, train_mol_dataset, val_mol_dataset):\n",
    "    depth = config[\"depth\"]\n",
    "    ffn_hidden_dim = config[\"ffn_hidden_dim\"]\n",
    "    ffn_num_layers = config[\"ffn_num_layers\"]\n",
    "    message_hidden_dim = config[\"message_hidden_dim\"]\n",
    "    batch_norm = config['batch_norm']\n",
    "\n",
    "    train_loader = cp.data.build_dataloader(train_mol_dataset, batch_size=32, num_workers=2, seed=RANDOM_SEED)\n",
    "    val_loader = cp.data.build_dataloader(val_mol_dataset, batch_size=32, num_workers=2, shuffle=False)\n",
    "\n",
    "    ###############################################################################################\n",
    "\n",
    "    mp = cp.nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "    agg = cp.nn.NormAggregation()\n",
    "    ffn_dims = mp.output_dim + train_mol_dataset.X_d.shape[-1]\n",
    "    ffn = cp.nn.BinaryClassificationFFN(n_tasks=1, input_dim=ffn_dims, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
    "    metric_list = [cp.nn.metrics.BinaryF1Score(), cp.nn.metrics.BinaryAUPRC(), cp.nn.metrics.BinaryAUROC()]\n",
    "    X_d_transform = cp.nn.ScaleTransform.from_standard_scaler(x_d_scaler)\n",
    "    mpnn = cp.models.MPNN(mp, agg, ffn, batch_norm, metric_list, X_d_transform=X_d_transform)\n",
    "\n",
    "    ################################################################################################\n",
    "    trainer = L.Trainer(\n",
    "        logger=None,\n",
    "        enable_checkpointing=True,\n",
    "        enable_progress_bar=False,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=20,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=False, patience=10),\n",
    "            TuneReportCheckpointCallback()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9637b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-10-25 03:24:50</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:05.12        </td></tr>\n",
       "<tr><td>Memory:      </td><td>30.8/503.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=18<br>Bracket: Iter 16.000: -0.4761234521865845 | Iter 8.000: -0.3978862762451172 | Iter 4.000: -0.3391759693622589 | Iter 2.000: -0.33652395009994507 | Iter 1.000: -0.3514161705970764<br>Logical resource usage: 4.0/30 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc             </th><th>batch_norm  </th><th style=\"text-align: right;\">  depth</th><th style=\"text-align: right;\">  ffn_hidden_dim</th><th style=\"text-align: right;\">  ffn_num_layers</th><th style=\"text-align: right;\">  message_hidden_dim</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_loss_step</th><th style=\"text-align: right;\">   val/f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_func_d41379f8</td><td>TERMINATED</td><td>172.17.0.3:38201</td><td>True        </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">            2300</td><td style=\"text-align: right;\">               3</td><td style=\"text-align: right;\">                1600</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">       176.255  </td><td style=\"text-align: right;\">   0.211957 </td><td style=\"text-align: right;\">        0.211957 </td><td style=\"text-align: right;\">0.543897 </td></tr>\n",
       "<tr><td>tune_func_ba5301c3</td><td>TERMINATED</td><td>172.17.0.3:38411</td><td>False       </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">            2200</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                1800</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        80.5885 </td><td style=\"text-align: right;\">   0.333499 </td><td style=\"text-align: right;\">        0.333499 </td><td style=\"text-align: right;\">0.563319 </td></tr>\n",
       "<tr><td>tune_func_cdc3619d</td><td>TERMINATED</td><td>172.17.0.3:39364</td><td>False       </td><td style=\"text-align: right;\">      6</td><td style=\"text-align: right;\">             700</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                 700</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">       123.208  </td><td style=\"text-align: right;\">   0.0496194</td><td style=\"text-align: right;\">        0.0496194</td><td style=\"text-align: right;\">0.577406 </td></tr>\n",
       "<tr><td>tune_func_13857fcf</td><td>TERMINATED</td><td>172.17.0.3:39534</td><td>False       </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">             900</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        68.4146 </td><td style=\"text-align: right;\">   0.344866 </td><td style=\"text-align: right;\">        0.344866 </td><td style=\"text-align: right;\">0.570806 </td></tr>\n",
       "<tr><td>tune_func_b7152dc2</td><td>TERMINATED</td><td>172.17.0.3:39705</td><td>True        </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            2000</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                1400</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.86239</td><td style=\"text-align: right;\">   0.418729 </td><td style=\"text-align: right;\">        0.418729 </td><td style=\"text-align: right;\">0.177632 </td></tr>\n",
       "<tr><td>tune_func_ccbdfd4b</td><td>TERMINATED</td><td>172.17.0.3:39886</td><td>True        </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">             600</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                2300</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.0917 </td><td style=\"text-align: right;\">   0.377505 </td><td style=\"text-align: right;\">        0.377505 </td><td style=\"text-align: right;\">0.188925 </td></tr>\n",
       "<tr><td>tune_func_3730a31c</td><td>TERMINATED</td><td>172.17.0.3:40083</td><td>False       </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">             500</td><td style=\"text-align: right;\">               3</td><td style=\"text-align: right;\">                1200</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        54.7908 </td><td style=\"text-align: right;\">   0.453117 </td><td style=\"text-align: right;\">        0.453117 </td><td style=\"text-align: right;\">0.561247 </td></tr>\n",
       "<tr><td>tune_func_d13ecb2a</td><td>TERMINATED</td><td>172.17.0.3:40450</td><td>False       </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">            2300</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                1700</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        30.4582 </td><td style=\"text-align: right;\">   0.206284 </td><td style=\"text-align: right;\">        0.206284 </td><td style=\"text-align: right;\">0.579909 </td></tr>\n",
       "<tr><td>tune_func_84ebe927</td><td>TERMINATED</td><td>172.17.0.3:40725</td><td>True        </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">             700</td><td style=\"text-align: right;\">               3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.3689 </td><td style=\"text-align: right;\">   0.416988 </td><td style=\"text-align: right;\">        0.416988 </td><td style=\"text-align: right;\">0.0143885</td></tr>\n",
       "<tr><td>tune_func_717630f7</td><td>TERMINATED</td><td>172.17.0.3:41252</td><td>False       </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            2300</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                 700</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        24.9933 </td><td style=\"text-align: right;\">   0.202179 </td><td style=\"text-align: right;\">        0.202179 </td><td style=\"text-align: right;\">0.581236 </td></tr>\n",
       "<tr><td>tune_func_6a06449c</td><td>TERMINATED</td><td>172.17.0.3:41430</td><td>False       </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">             800</td><td style=\"text-align: right;\">               3</td><td style=\"text-align: right;\">                1000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.57104</td><td style=\"text-align: right;\">   0.441866 </td><td style=\"text-align: right;\">        0.441866 </td><td style=\"text-align: right;\">0.522168 </td></tr>\n",
       "<tr><td>tune_func_7965d85a</td><td>TERMINATED</td><td>172.17.0.3:41750</td><td>True        </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">            2000</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                2400</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.4372 </td><td style=\"text-align: right;\">   0.432986 </td><td style=\"text-align: right;\">        0.432986 </td><td style=\"text-align: right;\">0.0763889</td></tr>\n",
       "<tr><td>tune_func_db9eb9bd</td><td>TERMINATED</td><td>172.17.0.3:41948</td><td>True        </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">            2000</td><td style=\"text-align: right;\">               3</td><td style=\"text-align: right;\">                1900</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        27.7018 </td><td style=\"text-align: right;\">   0.452473 </td><td style=\"text-align: right;\">        0.452473 </td><td style=\"text-align: right;\">0.0699301</td></tr>\n",
       "<tr><td>tune_func_ba90613d</td><td>TERMINATED</td><td>172.17.0.3:42142</td><td>True        </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">             500</td><td style=\"text-align: right;\">               3</td><td style=\"text-align: right;\">                1600</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        26.1266 </td><td style=\"text-align: right;\">   0.417816 </td><td style=\"text-align: right;\">        0.417816 </td><td style=\"text-align: right;\">0.0215054</td></tr>\n",
       "<tr><td>tune_func_c51617a0</td><td>TERMINATED</td><td>172.17.0.3:42317</td><td>False       </td><td style=\"text-align: right;\">      6</td><td style=\"text-align: right;\">            1500</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                1600</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.8137 </td><td style=\"text-align: right;\">   0.455433 </td><td style=\"text-align: right;\">        0.455433 </td><td style=\"text-align: right;\">0.542986 </td></tr>\n",
       "<tr><td>tune_func_32f06693</td><td>TERMINATED</td><td>172.17.0.3:42459</td><td>False       </td><td style=\"text-align: right;\">      6</td><td style=\"text-align: right;\">            1500</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                1600</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        40.804  </td><td style=\"text-align: right;\">   0.155194 </td><td style=\"text-align: right;\">        0.155194 </td><td style=\"text-align: right;\">0.521964 </td></tr>\n",
       "<tr><td>tune_func_4d2f444a</td><td>TERMINATED</td><td>172.17.0.3:42648</td><td>False       </td><td style=\"text-align: right;\">      6</td><td style=\"text-align: right;\">            1500</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                1600</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        37.0004 </td><td style=\"text-align: right;\">   0.186726 </td><td style=\"text-align: right;\">        0.186726 </td><td style=\"text-align: right;\">0.588785 </td></tr>\n",
       "<tr><td>tune_func_2c0d6570</td><td>TERMINATED</td><td>172.17.0.3:42862</td><td>False       </td><td style=\"text-align: right;\">      6</td><td style=\"text-align: right;\">            1500</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                1700</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        35.684  </td><td style=\"text-align: right;\">   0.191435 </td><td style=\"text-align: right;\">        0.191435 </td><td style=\"text-align: right;\">0.588517 </td></tr>\n",
       "<tr><td>tune_func_f238cdb7</td><td>TERMINATED</td><td>172.17.0.3:43015</td><td>False       </td><td style=\"text-align: right;\">      6</td><td style=\"text-align: right;\">            2400</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                 400</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        37.1806 </td><td style=\"text-align: right;\">   0.198401 </td><td style=\"text-align: right;\">        0.198401 </td><td style=\"text-align: right;\">0.589041 </td></tr>\n",
       "<tr><td>tune_func_d70e9e5d</td><td>TERMINATED</td><td>172.17.0.3:43201</td><td>False       </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">            2400</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                 400</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        28.9485 </td><td style=\"text-align: right;\">   0.19571  </td><td style=\"text-align: right;\">        0.19571  </td><td style=\"text-align: right;\">0.583908 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_func pid=38201)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m \n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 0 | message_passing | BondMessagePassing      | 5.4 M  | train\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 2 | bn              | BatchNorm1d             | 3.2 K  | train\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 14.8 M | train\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 20.1 M    Trainable params\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 20.1 M    Total params\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 80.584    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 30        Modules in train mode\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m \n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 0 | message_passing | BondMessagePassing      | 6.8 M  | train\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 9.3 M  | train\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 16.1 M    Trainable params\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 16.1 M    Total params\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 64.201    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 28        Modules in train mode\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d41379f8_1_batch_norm=True,depth=3,ffn_hidden_dim=2300,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-21-45/checkpoint_000000)\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m \n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 0 | message_passing | BondMessagePassing      | 1.1 M  | train\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 643 K  | train\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 1.7 M     Trainable params\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 1.7 M     Total params\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 6.938     Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_ba5301c3_2_batch_norm=False,depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=1800_2025-10-25_03-21-51/checkpoint_000000)\n",
      "2025-10-25 03:22:11,048\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m \n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 0 | message_passing | BondMessagePassing      | 815 K  | train\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.5 M  | train\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 2.4 M     Trainable params\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 2.4 M     Total params\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 9.454     Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 28        Modules in train mode\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d41379f8_1_batch_norm=True,depth=3,ffn_hidden_dim=2300,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-21-45/checkpoint_000001)\n",
      "2025-10-25 03:22:14,999\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_cdc3619d_3_batch_norm=False,depth=6,ffn_hidden_dim=700,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-21-57/checkpoint_000000)\n",
      "2025-10-25 03:22:16,921\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m \n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 0 | message_passing | BondMessagePassing      | 4.1 M  | train\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 2 | bn              | BatchNorm1d             | 2.8 K  | train\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 3.2 M  | train\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 7.4 M     Trainable params\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 7.4 M     Total params\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 29.534    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=39705)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "2025-10-25 03:22:21,673\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_13857fcf_4_batch_norm=False,depth=4,ffn_hidden_dim=900,ffn_num_layers=2,message_hidden_dim=600_2025-10-25_03-22-03/checkpoint_000000)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "2025-10-25 03:22:22,800\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:23,406\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "2025-10-25 03:22:25,129\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m \n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 0 | message_passing | BondMessagePassing      | 10.9 M | train\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 2 | bn              | BatchNorm1d             | 4.6 K  | train\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.5 M  | train\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 12.5 M    Trainable params\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 12.5 M    Total params\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 49.847    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "2025-10-25 03:22:26,797\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_ba5301c3_2_batch_norm=False,depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=1800_2025-10-25_03-21-51/checkpoint_000002)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-10-25 03:22:29,973\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m \n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 0 | message_passing | BondMessagePassing      | 3.1 M  | train\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.2 M  | train\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 4.3 M     Trainable params\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 4.3 M     Total params\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 17.125    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 30        Modules in train mode\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_13857fcf_4_batch_norm=False,depth=4,ffn_hidden_dim=900,ffn_num_layers=2,message_hidden_dim=600_2025-10-25_03-22-03/checkpoint_000001)\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_cdc3619d_3_batch_norm=False,depth=6,ffn_hidden_dim=700,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-21-57/checkpoint_000002)\n",
      "2025-10-25 03:22:32,734\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:35,147\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:36,788\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "2025-10-25 03:22:37,393\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m \n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 0 | message_passing | BondMessagePassing      | 6.1 M  | train\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 4.4 M  | train\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 10.5 M    Trainable params\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 10.5 M    Total params\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 41.856    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "2025-10-25 03:22:37,624\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=39886)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_ccbdfd4b_6_batch_norm=True,depth=5,ffn_hidden_dim=600,ffn_num_layers=1,message_hidden_dim=2300_2025-10-25_03-22-16/checkpoint_000000)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-10-25 03:22:38,685\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:41,416\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:43,712\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40450)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d13ecb2a_8_batch_norm=False,depth=2,ffn_hidden_dim=2300,ffn_num_layers=1,message_hidden_dim=1700_2025-10-25_03-22-28/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-10-25 03:22:44,561\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:45,995\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-10-25 03:22:47,187\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:47,282\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m \n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 0 | message_passing | BondMessagePassing      | 8.3 M  | train\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 2 | bn              | BatchNorm1d             | 4.0 K  | train\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 2.5 M  | train\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 10.9 M    Trainable params\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 10.9 M    Total params\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 43.427    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 30        Modules in train mode\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=40725)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "2025-10-25 03:22:50,153\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_cdc3619d_3_batch_norm=False,depth=6,ffn_hidden_dim=700,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-21-57/checkpoint_000004)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-10-25 03:22:50,734\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:51,436\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:54,000\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=38411)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_ba5301c3_2_batch_norm=False,depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=1800_2025-10-25_03-21-51/checkpoint_000005)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-10-25 03:22:58,235\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:58,799\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:59,128\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:59,277\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:22:59,481\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:00,310\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:01,942\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:05,432\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=40083)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_3730a31c_7_batch_norm=False,depth=3,ffn_hidden_dim=500,ffn_num_layers=3,message_hidden_dim=1200_2025-10-25_03-22-22/checkpoint_000004)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "2025-10-25 03:23:05,528\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:07,959\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:08,765\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m \n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 0 | message_passing | BondMessagePassing      | 1.1 M  | train\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 2.1 M  | train\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 3.2 M     Trainable params\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 3.2 M     Total params\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 12.820    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=39534)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_13857fcf_4_batch_norm=False,depth=4,ffn_hidden_dim=900,ffn_num_layers=2,message_hidden_dim=600_2025-10-25_03-22-03/checkpoint_000006)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-10-25 03:23:10,513\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:11,075\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:11,419\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:15,055\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_717630f7_10_batch_norm=False,depth=4,ffn_hidden_dim=2300,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-22-44/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m \n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 0 | message_passing | BondMessagePassing      | 2.2 M  | train\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 2.3 M  | train\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 4.4 M     Trainable params\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 4.4 M     Total params\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 17.663    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 30        Modules in train mode\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=41430)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "2025-10-25 03:23:16,652\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:17,486\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:18,671\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:18,693\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:20,886\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_717630f7_10_batch_norm=False,depth=4,ffn_hidden_dim=2300,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-22-44/checkpoint_000001)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-10-25 03:23:22,443\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:22,563\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:23,307\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:23,808\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:26,059\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=41252)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_717630f7_10_batch_norm=False,depth=4,ffn_hidden_dim=2300,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-22-44/checkpoint_000002)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m \n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 0 | message_passing | BondMessagePassing      | 11.9 M | train\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 2 | bn              | BatchNorm1d             | 4.8 K  | train\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 5.2 M  | train\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 17.1 M    Trainable params\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 17.1 M    Total params\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 68.578    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "2025-10-25 03:23:30,378\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:31,327\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d41379f8_1_batch_norm=True,depth=3,ffn_hidden_dim=2300,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-21-45/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-10-25 03:23:31,880\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m \n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 0 | message_passing | BondMessagePassing      | 7.5 M  | train\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 2 | bn              | BatchNorm1d             | 3.8 K  | train\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 12.2 M | train\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 19.8 M    Trainable params\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 19.8 M    Total params\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 79.072    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 30        Modules in train mode\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "2025-10-25 03:23:36,390\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=41750)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_7965d85a_12_batch_norm=True,depth=2,ffn_hidden_dim=2000,ffn_num_layers=1,message_hidden_dim=2400_2025-10-25_03-23-12/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-10-25 03:23:37,559\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:40,215\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:23:43,944\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_cdc3619d_3_batch_norm=False,depth=6,ffn_hidden_dim=700,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-21-57/checkpoint_000011)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d41379f8_1_batch_norm=True,depth=3,ffn_hidden_dim=2300,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-21-45/checkpoint_000010)\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=41948)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m \n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 0 | message_passing | BondMessagePassing      | 5.4 M  | train\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 2 | bn              | BatchNorm1d             | 3.2 K  | train\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.4 M  | train\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 6.8 M     Trainable params\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 6.8 M     Total params\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 27.152    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 30        Modules in train mode\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m \n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=39364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_cdc3619d_3_batch_norm=False,depth=6,ffn_hidden_dim=700,ffn_num_layers=1,message_hidden_dim=700_2025-10-25_03-21-57/checkpoint_000012)\n",
      "2025-10-25 03:23:50,645\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m GPU available: True (cuda), used: True\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m   warnings.warn(*args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m --------------------------------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 0 | message_passing | BondMessagePassing      | 5.4 M  | train\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 5.0 M  | train\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 10.4 M    Trainable params\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 10.4 M    Total params\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 41.418    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 28        Modules in train mode\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m \n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 0 | message_passing | BondMessagePassing      | 5.4 M  | train\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 5.0 M  | train\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 10.4 M    Trainable params\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 10.4 M    Total params\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 41.418    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 28        Modules in train mode\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=42317)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_c51617a0_15_batch_norm=False,depth=6,ffn_hidden_dim=1500,ffn_num_layers=2,message_hidden_dim=1600_2025-10-25_03-23-38/checkpoint_000000)\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m \n",
      "2025-10-25 03:23:59,877\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:00,082\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:01,209\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:04,571\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m   warnings.warn(*args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m --------------------------------------------------------------------\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 0 | message_passing | BondMessagePassing      | 5.4 M  | train\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 5.0 M  | train\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 10.4 M    Trainable params\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 10.4 M    Total params\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 41.418    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 28        Modules in train mode\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=42142)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_ba90613d_14_batch_norm=True,depth=3,ffn_hidden_dim=500,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-23-32/checkpoint_000000)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-10-25 03:24:07,436\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m \n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 0 | message_passing | BondMessagePassing      | 6.1 M  | train\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 5.1 M  | train\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 11.2 M    Trainable params\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 11.2 M    Total params\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 44.721    Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 28        Modules in train mode\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=42862)\u001b[0m --------------------------------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d41379f8_1_batch_norm=True,depth=3,ffn_hidden_dim=2300,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-21-45/checkpoint_000012)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-10-25 03:24:09,993\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:11,549\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "2025-10-25 03:24:14,188\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m \n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 0 | message_passing | BondMessagePassing      | 383 K  | train\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.5 M  | train\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 1.9 M     Trainable params\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 1.9 M     Total params\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 7.477     Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=43015)\u001b[0m --------------------------------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_func pid=42648)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_4d2f444a_17_batch_norm=False,depth=6,ffn_hidden_dim=1500,ffn_num_layers=2,message_hidden_dim=1600_2025-10-25_03-23-50/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-10-25 03:24:17,281\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d41379f8_1_batch_norm=True,depth=3,ffn_hidden_dim=2300,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-21-45/checkpoint_000013)\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m \n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m   | Name            | Type                    | Params | Mode \n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 0 | message_passing | BondMessagePassing      | 383 K  | train\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 1 | agg             | NormAggregation         | 0      | train\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 2 | bn              | Identity                | 0      | train\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.5 M  | train\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m --------------------------------------------------------------------\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 1.9 M     Trainable params\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 1.9 M     Total params\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 7.477     Total estimated model params size (MB)\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 26        Modules in train mode\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m /root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[36m(tune_func pid=38201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d41379f8_1_batch_norm=True,depth=3,ffn_hidden_dim=2300,ffn_num_layers=3,message_hidden_dim=1600_2025-10-25_03-21-45/checkpoint_000014)\n",
      "2025-10-25 03:24:26,842\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:27,787\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:29,154\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:31,478\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=42459)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_32f06693_16_batch_norm=False,depth=6,ffn_hidden_dim=1500,ffn_num_layers=2,message_hidden_dim=1600_2025-10-25_03-23-44/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-10-25 03:24:32,917\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:33,211\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:33,793\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:35,907\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:39,400\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d70e9e5d_20_batch_norm=False,depth=5,ffn_hidden_dim=2400,ffn_num_layers=1,message_hidden_dim=400_2025-10-25_03-24-11/checkpoint_000002)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-10-25 03:24:41,418\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:41,460\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:42,582\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(tune_func pid=43201)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tune_func_2025-10-25_03-21-38/tune_func_d70e9e5d_20_batch_norm=False,depth=5,ffn_hidden_dim=2400,ffn_num_layers=1,message_hidden_dim=400_2025-10-25_03-24-11/checkpoint_000003)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-10-25 03:24:46,821\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:48,965\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:49,222\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-10-25 03:24:50,185\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/tune_func_2025-10-25_03-21-38' in 0.0154s.\n",
      "2025-10-25 03:24:50,200\tINFO tune.py:1041 -- Total run time: 185.22 seconds (185.11 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"depth\": tune.qrandint(lower=2, upper=6, q=1),\n",
    "    \"ffn_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"ffn_num_layers\": tune.qrandint(lower=1, upper=3, q=1),\n",
    "    \"message_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"batch_norm\": tune.choice([True, False])\n",
    "}\n",
    "\n",
    "search_alg = ConcurrencyLimiter(OptunaSearch(seed=42), max_concurrent=8)\n",
    "scheduler = ASHAScheduler(max_t=20, grace_period=1, reduction_factor=2)\n",
    "\n",
    "tune_fn = tune.with_resources(\n",
    "    tune.with_parameters(\n",
    "        tune_func, \n",
    "        train_mol_dataset=train_mol_dataset, \n",
    "        val_mol_dataset=val_mol_dataset\n",
    "    ),\n",
    "    resources={\"CPU\": 4, \"GPU\": 0.25}\n",
    ")\n",
    "\n",
    "# Checkpoint config controls the checkpointing behavior of Ray\n",
    "checkpoint_config = tune.CheckpointConfig(\n",
    "    num_to_keep=1, # number of checkpoints to keep\n",
    "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
    "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune_fn,\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=20,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search_alg,\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"val_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "        failure_config = train.FailureConfig(max_failures=3)\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "_, best_result = results.get_best_result().best_checkpoints[0]\n",
    "best_config = best_result['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485f7f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.155194491147995,\n",
       " 'train_loss_step': 0.155194491147995,\n",
       " 'val/f1': 0.5219638347625732,\n",
       " 'val/prc': 0.6764230132102966,\n",
       " 'val/roc': 0.826703667640686,\n",
       " 'val_loss': 0.3367610573768616,\n",
       " 'train_loss_epoch': 0.36423495411872864,\n",
       " 'timestamp': 1761362671,\n",
       " 'checkpoint_dir_name': 'checkpoint_000001',\n",
       " 'should_checkpoint': True,\n",
       " 'done': False,\n",
       " 'training_iteration': 2,\n",
       " 'trial_id': '32f06693',\n",
       " 'date': '2025-10-25_03-24-31',\n",
       " 'time_this_iter_s': 19.863657474517822,\n",
       " 'time_total_s': 40.803967237472534,\n",
       " 'pid': 42459,\n",
       " 'hostname': '9a9bf6b9023e',\n",
       " 'node_ip': '172.17.0.3',\n",
       " 'config': {'depth': 6,\n",
       "  'ffn_hidden_dim': 1500,\n",
       "  'ffn_num_layers': 2,\n",
       "  'message_hidden_dim': 1600,\n",
       "  'batch_norm': False},\n",
       " 'time_since_restore': 40.803967237472534,\n",
       " 'iterations_since_restore': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9949b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type                    | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing      | 5.4 M  | train\n",
      "1 | agg             | NormAggregation         | 0      | train\n",
      "2 | bn              | Identity                | 0      | train\n",
      "3 | predictor       | BinaryClassificationFFN | 5.0 M  | train\n",
      "4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "5 | metrics         | ModuleList              | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.4 M    Total params\n",
      "41.418    Total estimated model params size (MB)\n",
      "28        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/root/delta/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/delta/tuning/wandb/run-20251025_032650-az7x346u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahul-e-dev/evaluation/runs/az7x346u' target=\"_blank\">devout-cloud-5</a></strong> to <a href='https://wandb.ai/rahul-e-dev/evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahul-e-dev/evaluation' target=\"_blank\">https://wandb.ai/rahul-e-dev/evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahul-e-dev/evaluation/runs/az7x346u' target=\"_blank\">https://wandb.ai/rahul-e-dev/evaluation/runs/az7x346u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/root/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "depth = best_config[\"depth\"]\n",
    "ffn_hidden_dim = best_config[\"ffn_hidden_dim\"]\n",
    "ffn_num_layers = best_config[\"ffn_num_layers\"]\n",
    "message_hidden_dim = best_config[\"message_hidden_dim\"]\n",
    "batch_norm = best_config['batch_norm']\n",
    "\n",
    "train_loader = cp.data.build_dataloader(train_mol_dataset, batch_size=32, num_workers=1, seed=RANDOM_SEED)\n",
    "val_loader = cp.data.build_dataloader(val_mol_dataset, batch_size=32, num_workers=1, shuffle=False)\n",
    "test_loader = cp.data.build_dataloader(test_mol_dataset, batch_size=32, num_workers=1, shuffle=False)\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "mp = cp.nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "agg = cp.nn.NormAggregation()\n",
    "ffn_dims = mp.output_dim + train_mol_dataset.X_d.shape[-1]\n",
    "ffn = cp.nn.BinaryClassificationFFN(n_tasks=1, input_dim=ffn_dims, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
    "metric_list = [cp.nn.metrics.BinaryF1Score(), cp.nn.metrics.BinaryAUPRC(), cp.nn.metrics.BinaryAUROC()]\n",
    "X_d_transform = cp.nn.ScaleTransform.from_standard_scaler(x_d_scaler)\n",
    "mpnn = cp.models.MPNN(mp, agg, ffn, batch_norm, metric_list, X_d_transform=X_d_transform)\n",
    "\n",
    "################################################################################################\n",
    "trainer = L.Trainer(\n",
    "    logger=None,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=False,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=False, patience=10),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "mpnn = cp.models.MPNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "run = wandb.init(project=\"evaluation\")\n",
    "wandb.mark_preempting()\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    enable_progress_bar=False,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "test_ds_preds = trainer.predict(model=mpnn, dataloaders=test_loader)\n",
    "test_ds_preds = torch.cat(test_ds_preds)\n",
    "\n",
    "pred_probs = test_ds_preds.squeeze().numpy()\n",
    "preds = (pred_probs >= 0.5).astype(float)\n",
    "labels = df_test['per_inhibition'] > 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41fb9794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-cloud-5</strong> at: <a href='https://wandb.ai/rahul-e-dev/evaluation/runs/az7x346u' target=\"_blank\">https://wandb.ai/rahul-e-dev/evaluation/runs/az7x346u</a><br> View project at: <a href='https://wandb.ai/rahul-e-dev/evaluation' target=\"_blank\">https://wandb.ai/rahul-e-dev/evaluation</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251025_032650-az7x346u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "run.log({\n",
    "    'final_metrics': wandb.Table(\n",
    "        columns=['accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'AUCROC', 'PRAUC'],\n",
    "        data=[[\n",
    "            accuracy_score(labels, preds),\n",
    "            balanced_accuracy_score(labels, preds),\n",
    "            f1_score(labels, preds),\n",
    "            precision_score(labels, preds),\n",
    "            recall_score(labels, preds),\n",
    "            roc_auc_score(labels, pred_probs),\n",
    "            average_precision_score(labels, pred_probs)\n",
    "        ]]\n",
    "    )\n",
    "})\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6373d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
