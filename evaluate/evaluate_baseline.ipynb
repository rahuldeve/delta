{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce41214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chemprop as cp\n",
    "import torch\n",
    "from glob import glob\n",
    "import lightning as L\n",
    "from tempfile import TemporaryDirectory\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_datapoint(row):\n",
    "    feat_entry_names = [f for f in row.index if f.startswith('feat')]\n",
    "    feat_array = pd.to_numeric(row[feat_entry_names], errors=\"coerce\")\n",
    "    return cp.data.MoleculeDatapoint(\n",
    "        mol=row['mol'],\n",
    "        y=np.array([row['per_inhibition'] > 50]),\n",
    "        x_d=feat_array.to_numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_on_split(df_train, df_val, df_test):\n",
    "    df_train = df_train.copy()\n",
    "    df_val = df_val.copy()\n",
    "    df_test = df_test.copy()\n",
    "\n",
    "    df_train['mol'] = df_train['mol_ser'].map(pickle.loads)\n",
    "    df_val['mol'] = df_val['mol_ser'].map(pickle.loads)\n",
    "    df_test['mol'] = df_test['mol_ser'].map(pickle.loads)\n",
    "\n",
    "    featurizer = cp.featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "    train_mol_dataset = cp.data.MoleculeDataset(df_train.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "    val_mol_dataset = cp.data.MoleculeDataset(df_val.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "    test_mol_dataset = cp.data.MoleculeDataset(df_test.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "\n",
    "    x_d_scaler = train_mol_dataset.normalize_inputs(\"X_d\")\n",
    "    val_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "    test_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "\n",
    "    train_mol_dataset.cache = True\n",
    "    val_mol_dataset.cache = True\n",
    "    test_mol_dataset.cache = True\n",
    "\n",
    "    train_loader = cp.data.build_dataloader(train_mol_dataset, batch_size=32, num_workers=8, seed=RANDOM_SEED)\n",
    "    val_loader = cp.data.build_dataloader(val_mol_dataset, batch_size=32, num_workers=8, shuffle=False)\n",
    "    test_loader = cp.data.build_dataloader(test_mol_dataset, batch_size=32, num_workers=8, shuffle=False)\n",
    "\n",
    "    ###############################################################################################\n",
    "\n",
    "    fdims = cp.featurizers.SimpleMoleculeMolGraphFeaturizer().shape # the dimensions of the featurizer, given as (atom_dims, bond_dims).\n",
    "    mp = cp.nn.BondMessagePassing()\n",
    "    agg = cp.nn.NormAggregation()\n",
    "    ffn_dims = mp.output_dim + len([f for f in df_train.columns if f.startswith(\"feat\")])\n",
    "    ffn = cp.nn.BinaryClassificationFFN(n_tasks=1, input_dim=ffn_dims)\n",
    "    batch_norm = True\n",
    "    metric_list = [cp.nn.metrics.BinaryF1Score(), cp.nn.metrics.BinaryAUPRC(), cp.nn.metrics.BinaryAUROC()]\n",
    "    X_d_transform = cp.nn.ScaleTransform.from_standard_scaler(x_d_scaler)\n",
    "    mpnn = cp.models.MPNN(mp, agg, ffn, batch_norm, metric_list, X_d_transform=X_d_transform)\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    with TemporaryDirectory() as tmpdir:\n",
    "        trainer = L.Trainer(\n",
    "            logger=None,\n",
    "            enable_checkpointing=True,  # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "            enable_progress_bar=False,\n",
    "            accelerator=\"auto\",\n",
    "            devices=1,\n",
    "            max_epochs=50,  # number of epochs to train for\n",
    "            default_root_dir=tmpdir,\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=10),\n",
    "                ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainer.fit(mpnn, train_loader, val_loader)\n",
    "\n",
    "        mpnn = cp.models.MPNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "\n",
    "    ##################################################################################################\n",
    "    trainer = L.Trainer(\n",
    "        enable_progress_bar=False,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "    )\n",
    "\n",
    "    test_ds_preds = trainer.predict(model=mpnn, dataloaders=test_loader)\n",
    "    test_ds_preds = torch.cat(test_ds_preds)\n",
    "\n",
    "    pred_probs = test_ds_preds.squeeze().numpy()\n",
    "    preds = (pred_probs >= 0.5).astype(float)\n",
    "    labels = df_test['per_inhibition'] > 50.0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(labels, preds),\n",
    "        \"f1_score\": f1_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"roc_auc\": roc_auc_score(labels, pred_probs),\n",
    "        \"average_precision\": average_precision_score(labels, pred_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6d58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul-e-dev\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rahul/delta/evaluate/wandb/run-20251018_105747-sm1pkbke</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahul-e-dev/evaluation/runs/sm1pkbke' target=\"_blank\">eager-gorge-7</a></strong> to <a href='https://wandb.ai/rahul-e-dev/evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahul-e-dev/evaluation' target=\"_blank\">https://wandb.ai/rahul-e-dev/evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahul-e-dev/evaluation/runs/sm1pkbke' target=\"_blank\">https://wandb.ai/rahul-e-dev/evaluation/runs/sm1pkbke</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c596100346ba42eeb649f267e7d4dd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                    | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing      | 227 K  | train\n",
      "1 | agg             | NormAggregation         | 0      | train\n",
      "2 | bn              | BatchNorm1d             | 600    | train\n",
      "3 | predictor       | BinaryClassificationFFN | 155 K  | train\n",
      "4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "5 | metrics         | ModuleList              | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "384 K     Trainable params\n",
      "0         Non-trainable params\n",
      "384 K     Total params\n",
      "1.536     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "Metric val_loss improved. New best score: 0.683\n",
      "Metric val_loss improved by 0.044 >= min_delta = 0.0. New best score: 0.639\n",
      "Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.585\n",
      "Metric val_loss improved by 0.034 >= min_delta = 0.0. New best score: 0.551\n",
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.542\n",
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.542. Signaling Trainer to stop.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed_0x4\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                    | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing      | 227 K  | train\n",
      "1 | agg             | NormAggregation         | 0      | train\n",
      "2 | bn              | BatchNorm1d             | 600    | train\n",
      "3 | predictor       | BinaryClassificationFFN | 155 K  | train\n",
      "4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "5 | metrics         | ModuleList              | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "384 K     Trainable params\n",
      "0         Non-trainable params\n",
      "384 K     Total params\n",
      "1.536     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "Metric val_loss improved. New best score: 0.681\n",
      "Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.643\n",
      "Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.587\n",
      "Metric val_loss improved by 0.034 >= min_delta = 0.0. New best score: 0.553\n",
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.537\n",
      "Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.510\n",
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.510. Signaling Trainer to stop.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed_2x3\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                    | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing      | 227 K  | train\n",
      "1 | agg             | NormAggregation         | 0      | train\n",
      "2 | bn              | BatchNorm1d             | 600    | train\n",
      "3 | predictor       | BinaryClassificationFFN | 155 K  | train\n",
      "4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "5 | metrics         | ModuleList              | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "384 K     Trainable params\n",
      "0         Non-trainable params\n",
      "384 K     Total params\n",
      "1.536     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "Metric val_loss improved. New best score: 0.691\n",
      "Metric val_loss improved by 0.066 >= min_delta = 0.0. New best score: 0.625\n",
      "Metric val_loss improved by 0.063 >= min_delta = 0.0. New best score: 0.563\n",
      "Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.517\n",
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.493\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.491\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.489\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.486\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.476\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.459\n",
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.435\n",
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.435. Signaling Trainer to stop.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed_0x3\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"evaluation\")\n",
    "wandb.mark_preempting()\n",
    "\n",
    "\n",
    "cross_val_results = []\n",
    "for split_fpath in tqdm(glob(\"./generated_splits/*.parquet\")):\n",
    "    matches = re.match(\".*split_(?P<outer>\\\\d)x(?P<inner>\\\\d)\", split_fpath)\n",
    "    assert matches is not None, split_fpath\n",
    "    matches = matches.groupdict()\n",
    "    outer_idx, inner_idx = int(matches[\"outer\"]), int(matches[\"inner\"]) \n",
    "\n",
    "    total_split_df = pd.read_parquet(split_fpath)\n",
    "    total_split_df = total_split_df.drop(\"index\", axis=1)\n",
    "\n",
    "    df_train = total_split_df[total_split_df['split'] == \"train\"]\n",
    "    df_val = total_split_df[total_split_df['split'] == \"val\"]\n",
    "    df_test = total_split_df[total_split_df['split'] == \"test\"]\n",
    "\n",
    "    df_train = df_train.drop(\"split\", axis=1)\n",
    "    df_val = df_val.drop(\"split\", axis=1)\n",
    "    df_test = df_test.drop(\"split\", axis=1)\n",
    "\n",
    "    scores = evaluate_on_split(df_train, df_val, df_test)\n",
    "    split_result_entry = scores | {\"outer\": outer_idx, \"inner\": inner_idx}\n",
    "    cross_val_results.append(split_result_entry)\n",
    "\n",
    "    print(f\"completed_{outer_idx}x{inner_idx}\")\n",
    "    print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed26cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results = pd.DataFrame.from_records(cross_val_results)\n",
    "cross_val_results[\"model\"] = \"baseline\"\n",
    "run.log({\"Cross Val Results\": wandb.Table(dataframe=cross_val_results)})\n",
    "\n",
    "mean_scores = cross_val_results.drop([\"outer\", \"inner\"], axis=1).groupby(\"model\").agg(\"mean\").reset_index()\n",
    "mean_scores.columns = [f\"mean_{c}\" for c in mean_scores.columns]\n",
    "run.log({\"Mean Results\": wandb.Table(dataframe=cross_val_results)})\n",
    "\n",
    "std_scores = cross_val_results.drop([\"outer\", \"inner\"], axis=1).groupby(\"model\").agg(\"std\").reset_index()\n",
    "std_scores.columns = [f\"std_{c}\" for c in std_scores.columns]\n",
    "run.log({\"Std Results\": wandb.Table(dataframe=cross_val_results)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5dd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7e7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
