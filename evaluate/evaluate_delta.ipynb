{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cfe4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chemprop as cp\n",
    "import torch\n",
    "from glob import glob\n",
    "import lightning as L\n",
    "from tempfile import TemporaryDirectory\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import random\n",
    "from typing import NamedTuple, Iterable\n",
    "from itertools import chain\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ac87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPairDataPoint(NamedTuple):\n",
    "    anchor: cp.data.datasets.Datum\n",
    "    exemplar: list[cp.data.datasets.Datum]\n",
    "    random: list[cp.data.datasets.Datum]\n",
    "\n",
    "\n",
    "class RandomPairTrainBatch(NamedTuple):\n",
    "    anchor: cp.data.collate.TrainingBatch\n",
    "    exemplar: cp.data.collate.TrainingBatch\n",
    "    random: cp.data.collate.TrainingBatch\n",
    "    B: int\n",
    "    C: int\n",
    "    \n",
    "\n",
    "class RandomPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mol_dataset, n_candidates):\n",
    "        super().__init__()\n",
    "        self.mol_dataset: cp.data.datasets.MoleculeDataset = mol_dataset\n",
    "        self.n_candidates: int = n_candidates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mol_dataset)\n",
    "    \n",
    "    def get_exemplar_candidates(self):\n",
    "        targets = self.mol_dataset.Y.squeeze()\n",
    "        mask = targets > 50\n",
    "        weights = np.where(mask, 1.0, 0.0)\n",
    "        probs = weights / weights.sum()\n",
    "        exemplar_idxs = np.random.choice(\n",
    "            targets.shape[0], \n",
    "            size=(self.n_candidates,), \n",
    "            p=probs, \n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        return [self.mol_dataset[idx] for idx in exemplar_idxs]\n",
    "\n",
    "    \n",
    "    def get_random_candidates(self):\n",
    "        targets = self.mol_dataset.Y.squeeze()\n",
    "        candidate_idxs = np.random.choice(\n",
    "            targets.shape[0], \n",
    "            size=(self.n_candidates,), \n",
    "            replace=False\n",
    "        )\n",
    "        return [self.mol_dataset[idx] for idx in candidate_idxs]\n",
    "\n",
    "    def __getitem__(self, idx) -> RandomPairDataPoint:\n",
    "        return RandomPairDataPoint(\n",
    "            self.mol_dataset[idx], \n",
    "            self.get_exemplar_candidates(),\n",
    "            self.get_random_candidates()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_function(batch):\n",
    "        batch_anchors, batch_exemplars, batch_candidates = zip(*batch)\n",
    "        B = len(batch)\n",
    "        C = len(batch_candidates[0])\n",
    "        batch_anchors = cp.data.dataloader.collate_batch(batch_anchors)\n",
    "        batch_exemplars = cp.data.dataloader.collate_batch(chain.from_iterable(batch_exemplars))\n",
    "        batch_candidates = cp.data.dataloader.collate_batch(chain.from_iterable(batch_candidates))\n",
    "        return RandomPairTrainBatch(batch_anchors, batch_exemplars, batch_candidates, B, C)\n",
    "    \n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class RandomPairDataModule(L.LightningDataModule):\n",
    "    def __init__(self, mol_ds_train, mol_ds_val) -> None:\n",
    "        super().__init__()\n",
    "        self.mol_ds_train: cp.data.MoleculeDataset = mol_ds_train\n",
    "        self.mol_ds_val: cp.data.MoleculeDataset = mol_ds_val\n",
    "        self.batch_size=32\n",
    "        self.candidate_size=8\n",
    "\n",
    "        self.ds_train = None\n",
    "        self.ds_val = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds_train = RandomPairDataset(self.mol_ds_train, self.candidate_size)\n",
    "        self.ds_val = RandomPairDataset(self.mol_ds_val, self.candidate_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        assert self.ds_train is not None\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=RandomPairDataset.collate_function,\n",
    "            worker_init_fn=seed_worker,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        assert self.ds_val is not None\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.ds_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=RandomPairDataset.collate_function,\n",
    "            num_workers=8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1235938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESCALInteraction(torch.nn.Module):\n",
    "    def __init__(self, ndims) -> None:\n",
    "        super().__init__()\n",
    "        self.interaction_matrix = torch.nn.Linear(ndims, ndims, bias=False)\n",
    "        self.head_dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, head_emb, tail_emb):\n",
    "        R = self.interaction_matrix.weight.unsqueeze(0)\n",
    "        z = self.head_dropout(head_emb @ R) @ tail_emb.transpose(-2, -1)\n",
    "        return z.squeeze()\n",
    "    \n",
    "\n",
    "class ContrastiveMPNN(cp.models.MPNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: cp.nn.MessagePassing,\n",
    "        agg: cp.nn.Aggregation,\n",
    "        predictor: cp.nn.Predictor,\n",
    "        batch_norm: bool = False,\n",
    "        metrics: Iterable[cp.nn.ChempropMetric] | None = None,\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 0.0001,\n",
    "        max_lr: float = 0.001,\n",
    "        final_lr: float = 0.0001,\n",
    "        X_d_transform: cp.nn.transforms.ScaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message_passing,\n",
    "            agg,\n",
    "            predictor,\n",
    "            batch_norm,\n",
    "            metrics,\n",
    "            warmup_epochs,\n",
    "            init_lr,\n",
    "            max_lr,\n",
    "            final_lr,\n",
    "            X_d_transform,\n",
    "        )\n",
    "\n",
    "        self.interaction = RESCALInteraction(300)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    def embed_simple_batch(self, batch: cp.data.collate.TrainingBatch):\n",
    "        bmg, V_d, X_d, target, _, _, _ = batch\n",
    "        Z = self.encoding(bmg, V_d, X_d)\n",
    "        return dict(embeds=Z, targets=target)\n",
    "    \n",
    "\n",
    "    def get_losses(self, batch: RandomPairTrainBatch):\n",
    "        B, C = batch.B, batch.C\n",
    "\n",
    "        bmg, V_d, X_d, target_anchor, _, _, _ = batch.anchor\n",
    "        Z_anchor = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        bmg, V_d, X_d, target_exemplar, _, _, _ = batch.exemplar\n",
    "        Z_exemplar = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        bmg, V_d, X_d, target_random, _, _, _ = batch.random\n",
    "        Z_random = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        Z_anchor = Z_anchor.view(B, 1, -1)              # (B, d) -> (B, 1, d)\n",
    "        Z_exemplar = Z_exemplar.view(B, C, -1)          # (B*C, d) -> (B, C, d)\n",
    "        Z_random = Z_random.view(B, C, -1)              # (B*C, d) -> (B, C, d)\n",
    "        \n",
    "        target_anchor = target_anchor.view(-1, 1)\n",
    "        target_exemplar = target_exemplar.view(B, C)\n",
    "        target_random = target_random.view(B, C)\n",
    "\n",
    "\n",
    "        # left to right loss\n",
    "        preds = self.interaction(Z_anchor, Z_exemplar).squeeze()\n",
    "        labels = (target_anchor > target_exemplar).float() # type: ignore\n",
    "        lr_exemplar_loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        preds = self.interaction(Z_anchor, Z_random).squeeze()\n",
    "        labels = (target_anchor > target_random).float() # type: ignore\n",
    "        lr_random_loss = self.loss_fn(preds, labels)\n",
    "       \n",
    "\n",
    "        # right to left loss\n",
    "        preds = self.interaction(Z_exemplar, Z_anchor).squeeze()\n",
    "        labels = (target_exemplar > target_anchor).float() # type: ignore\n",
    "        rl_exemplar_loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        preds = self.interaction(Z_random, Z_anchor).squeeze()\n",
    "        labels = (target_random > target_anchor).float() # type: ignore\n",
    "        rl_random_loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        loss = (lr_exemplar_loss + lr_random_loss + rl_exemplar_loss + rl_random_loss) / 4\n",
    "        return loss, (lr_exemplar_loss, lr_random_loss, rl_exemplar_loss, rl_random_loss)\n",
    "\n",
    "\n",
    "    def training_step(self, batch: RandomPairTrainBatch, batch_idx):  # type: ignore\n",
    "        loss, (lr_exemplar_loss, lr_random_loss, rl_exemplar_loss, rl_random_loss) = self.get_losses(batch)\n",
    "        self.log(\"train_lr_exemplar_loss\", lr_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_lr_random_loss\", lr_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_rl_exemplar_loss\", rl_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_rl_random_loss\", rl_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_loss\", loss, batch_size=batch.B, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: RandomPairTrainBatch, batch_idx):  # type: ignore\n",
    "        loss, (lr_exemplar_loss, lr_random_loss, rl_exemplar_loss, rl_random_loss) = self.get_losses(batch)\n",
    "        self.log(\"val_lr_exemplar_loss\", lr_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_lr_random_loss\", lr_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_rl_exemplar_loss\", rl_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_rl_random_loss\", rl_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_loss\", loss, batch_size=batch.B)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f2dbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_datapoint(row):\n",
    "    feat_entry_names = [f for f in row.index if f.startswith('feat')]\n",
    "    feat_array = pd.to_numeric(row[feat_entry_names], errors=\"coerce\")\n",
    "    return cp.data.MoleculeDatapoint(\n",
    "        mol=row['mol'], \n",
    "        y=np.array([row['per_inhibition']]),\n",
    "        x_d=feat_array.to_numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_all(mol_dataset: cp.data.datasets.MoleculeDataset, contrastive_mpnn):\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        mol_dataset, \n",
    "        batch_size=64, \n",
    "        shuffle=False, \n",
    "        collate_fn=cp.data.dataloader.collate_batch\n",
    "    )\n",
    "    all_embeds = []\n",
    "    for batch in dl:\n",
    "        batch = move_data_to_device(batch, contrastive_mpnn.device)\n",
    "        res = contrastive_mpnn.embed_simple_batch(batch)\n",
    "        all_embeds.append(res['embeds'])\n",
    "\n",
    "    all_embeds = torch.cat(all_embeds)\n",
    "    return all_embeds\n",
    "\n",
    "\n",
    "def evaluate_on_split(df_train, df_val, df_test):\n",
    "    df_train = df_train.copy().sample(100).reset_index(drop=True)\n",
    "    df_val = df_val.copy().sample(100).reset_index(drop=True)\n",
    "    df_test = df_test.copy().sample(100).reset_index(drop=True)\n",
    "\n",
    "    df_train['mol'] = df_train['mol_ser'].map(pickle.loads)\n",
    "    df_val['mol'] = df_val['mol_ser'].map(pickle.loads)\n",
    "    df_test['mol'] = df_test['mol_ser'].map(pickle.loads)\n",
    "\n",
    "    featurizer = cp.featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "    train_mol_dataset = cp.data.MoleculeDataset(df_train.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "    val_mol_dataset = cp.data.MoleculeDataset(df_val.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "    test_mol_dataset = cp.data.MoleculeDataset(df_test.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "\n",
    "    x_d_scaler = train_mol_dataset.normalize_inputs(\"X_d\")\n",
    "    val_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "    test_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "\n",
    "    train_mol_dataset.cache = True\n",
    "    val_mol_dataset.cache = True\n",
    "    test_mol_dataset.cache = True\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    fdims = cp.featurizers.SimpleMoleculeMolGraphFeaturizer().shape # the dimensions of the featurizer, given as (atom_dims, bond_dims).\n",
    "    mp = cp.nn.BondMessagePassing()\n",
    "    agg = cp.nn.NormAggregation()\n",
    "    ffn_dims = mp.output_dim + len([f for f in df_train.columns if f.startswith(\"feat\")])\n",
    "    ffn = cp.nn.BinaryClassificationFFN(n_tasks=1, input_dim=ffn_dims, activation=torch.nn.ELU())\n",
    "    batch_norm = True\n",
    "    metric_list = [cp.nn.metrics.BinaryF1Score(), cp.nn.metrics.BinaryAUPRC(), cp.nn.metrics.BinaryAUROC()]\n",
    "    X_d_transform = cp.nn.ScaleTransform.from_standard_scaler(x_d_scaler)\n",
    "    contrastive_mpnn = ContrastiveMPNN(mp, agg, ffn, batch_norm, metric_list, X_d_transform=X_d_transform)\n",
    "\n",
    "\n",
    "\n",
    "    # with TemporaryDirectory() as tmpdir:\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        logger=None,\n",
    "        enable_checkpointing=True,\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=1,\n",
    "        # default_root_dir=tmpdir,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=10),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainer.fit(contrastive_mpnn, datamodule=RandomPairDataModule(train_mol_dataset, val_mol_dataset))\n",
    "\n",
    "    ckpt = torch.load(trainer.checkpoint_callback.best_model_path, map_location='cuda', weights_only=False)\n",
    "    contrastive_mpnn.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    train_embeds = embed_all(train_mol_dataset, contrastive_mpnn)\n",
    "    test_embeds = embed_all(test_mol_dataset, contrastive_mpnn)\n",
    "\n",
    "\n",
    "    exemplar_idxs = np.argwhere(train_mol_dataset.Y.squeeze() > 50)\n",
    "    exemplar_embeds = train_embeds[exemplar_idxs].squeeze()\n",
    "    # exemplar_targets = train_mol_dataset.Y[exemplar_idxs].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_probs = contrastive_mpnn.interaction(test_embeds, exemplar_embeds).sigmoid().mean(axis=-1)\n",
    "        preds = (pred_probs >= 0.5).float()\n",
    "\n",
    "        pred_probs = pred_probs.detach().numpy().squeeze()\n",
    "        preds = preds.detach().numpy().squeeze()\n",
    "        labels = df_test['per_inhibition'] > 50.0\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(labels, preds),\n",
    "        \"f1_score\": f1_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"roc_auc\": roc_auc_score(labels, pred_probs),\n",
    "        \"average_precision\": average_precision_score(labels, pred_probs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ae9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"evaluation\")\n",
    "wandb.mark_preempting()\n",
    "\n",
    "\n",
    "cross_val_results = []\n",
    "for split_fpath in tqdm(glob(\"./generated_splits/*.parquet\")):\n",
    "    matches = re.match(\".*split_(?P<outer>\\\\d)x(?P<inner>\\\\d)\", split_fpath)\n",
    "    assert matches is not None, split_fpath\n",
    "    matches = matches.groupdict()\n",
    "    outer_idx, inner_idx = int(matches[\"outer\"]), int(matches[\"inner\"]) \n",
    "\n",
    "    total_split_df = pd.read_parquet(split_fpath)\n",
    "    total_split_df = total_split_df.drop(\"index\", axis=1)\n",
    "\n",
    "    df_train = total_split_df[total_split_df['split'] == \"train\"]\n",
    "    df_val = total_split_df[total_split_df['split'] == \"val\"]\n",
    "    df_test = total_split_df[total_split_df['split'] == \"test\"]\n",
    "\n",
    "    df_train = df_train.drop(\"split\", axis=1)\n",
    "    df_val = df_val.drop(\"split\", axis=1)\n",
    "    df_test = df_test.drop(\"split\", axis=1)\n",
    "\n",
    "    scores = evaluate_on_split(df_train, df_val, df_test)\n",
    "    split_result_entry = scores | {\"outer\": outer_idx, \"inner\": inner_idx}\n",
    "    cross_val_results.append(split_result_entry)\n",
    "\n",
    "    print(f\"completed_{outer_idx}x{inner_idx}\")\n",
    "    print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83774033",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results = pd.DataFrame.from_records(cross_val_results)\n",
    "cross_val_results[\"model\"] = \"baseline\"\n",
    "run.log({\"Cross Val Results\": wandb.Table(dataframe=cross_val_results)})\n",
    "\n",
    "mean_scores = cross_val_results.drop([\"outer\", \"inner\"], axis=1).groupby(\"model\").agg(\"mean\").reset_index()\n",
    "mean_scores.columns = [f\"mean_{c}\" for c in mean_scores.columns]\n",
    "run.log({\"Mean Results\": wandb.Table(dataframe=cross_val_results)})\n",
    "\n",
    "std_scores = cross_val_results.drop([\"outer\", \"inner\"], axis=1).groupby(\"model\").agg(\"std\").reset_index()\n",
    "std_scores.columns = [f\"std_{c}\" for c in std_scores.columns]\n",
    "run.log({\"Std Results\": wandb.Table(dataframe=cross_val_results)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19fae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
