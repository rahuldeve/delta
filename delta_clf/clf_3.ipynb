{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2befbca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/rahul_e_dev/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul-e-dev\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from chemprop import data, featurizers, models, nn\n",
    "from data import ConstrastiveDataModule, ExemplarDataset\n",
    "from dotenv import load_dotenv\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "from commons.data import load_and_split_gsk_dataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "load_dotenv('.env.secret')\n",
    "wandb.login(key='cf344975eb80edf6f0d52af80528cc6094234caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6646a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_and_split_gsk_dataset(\"../GSK_HepG2.csv\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09fd022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.conf import DEFAULT_HIDDEN_DIM\n",
    "from chemprop.nn.ffn import MLP\n",
    "from chemprop.nn.metrics import BCELoss, BinaryAUPRC, ChempropMetric\n",
    "from chemprop.nn.predictors import Predictor, PredictorRegistry\n",
    "from chemprop.nn.transforms import UnscaleTransform\n",
    "from chemprop.utils import Factory\n",
    "from lightning.pytorch.core.mixins import HyperparametersMixin\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "@PredictorRegistry.register(\"ranking\")\n",
    "class RankNetPredictor(Predictor, HyperparametersMixin):\n",
    "\n",
    "    n_targets = 1\n",
    "    _T_default_criterion = BCELoss\n",
    "    _T_default_metric = BinaryAUPRC\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_tasks: int = 1,\n",
    "        input_dim: int = DEFAULT_HIDDEN_DIM,\n",
    "        hidden_dim: int = 300,\n",
    "        n_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        activation: str | torch.nn.Module = \"relu\",\n",
    "        criterion: ChempropMetric | None = None,\n",
    "        task_weights: Tensor | None = None,\n",
    "        threshold: float | None = None,\n",
    "        output_transform: UnscaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # manually add criterion and output_transform to hparams to suppress lightning's warning\n",
    "        # about double saving their state_dict values.\n",
    "        ignore_list = [\"criterion\", \"output_transform\", \"activation\"]\n",
    "        self.save_hyperparameters(ignore=ignore_list)\n",
    "        self.hparams[\"criterion\"] = criterion\n",
    "        self.hparams[\"output_transform\"] = output_transform\n",
    "        self.hparams[\"activation\"] = activation\n",
    "        self.hparams[\"cls\"] = self.__class__\n",
    "\n",
    "        self.ffn = MLP.build(\n",
    "            input_dim, n_tasks * self.n_targets, hidden_dim, n_layers, dropout, activation\n",
    "        )\n",
    "        task_weights = torch.ones(n_tasks) if task_weights is None else task_weights\n",
    "        self.criterion = criterion or Factory.build(\n",
    "            self._T_default_criterion, task_weights=task_weights, threshold=threshold\n",
    "        )\n",
    "        self.output_transform = output_transform if output_transform is not None else torch.nn.Identity()\n",
    "\n",
    "    @property\n",
    "    def input_dim(self) -> int:\n",
    "        return self.ffn.input_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return self.ffn.output_dim\n",
    "\n",
    "    @property\n",
    "    def n_tasks(self) -> int:\n",
    "        return self.output_dim // self.n_targets\n",
    "\n",
    "    def forward(self, Z: Tensor) -> Tensor:\n",
    "        # print(Z.shape)\n",
    "        A, B = torch.split(Z, self.input_dim, dim=-1)\n",
    "        logit_A = self.ffn(A)\n",
    "        logit_B = self.ffn(B)\n",
    "        return (logit_A - logit_B).sigmoid()\n",
    "\n",
    "    def encode(self, Z: Tensor, i: int) -> Tensor:\n",
    "        A, B = torch.split(Z, self.input_dim, dim=-1)\n",
    "        enc_A = self.ffn[:i](A)\n",
    "        enc_B = self.ffn[:i](B)\n",
    "        return torch.cat([enc_A, enc_B], dim=-1)\n",
    "    \n",
    "    def train_step(self, Z: Tensor) -> Tensor:\n",
    "        # print(Z.shape)\n",
    "        A, B = torch.split(Z, self.input_dim, dim=-1)\n",
    "        logit_A = self.ffn(A)\n",
    "        logit_B = self.ffn(B)\n",
    "        return logit_A - logit_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53dcdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdims = featurizers.SimpleMoleculeMolGraphFeaturizer().shape # the dimensions of the featurizer, given as (atom_dims, bond_dims).\n",
    "mcmp = nn.MulticomponentMessagePassing(\n",
    "    blocks=[nn.BondMessagePassing(*fdims)],\n",
    "    n_components=2,\n",
    "    shared=True\n",
    ")\n",
    "agg = nn.NormAggregation()\n",
    "ffn = RankNetPredictor(n_tasks=1, input_dim=mcmp.output_dim // 2)\n",
    "batch_norm = True\n",
    "metric_list = [nn.metrics.BinaryF1Score(), nn.metrics.BinaryAUPRC(), nn.metrics.BinaryAUROC()]\n",
    "mpnn = models.multi.MulticomponentMPNN(mcmp, agg, ffn, batch_norm, metric_list)\n",
    "mpnn.max_lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151275e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20251010_221442-4j01bn3i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/4j01bn3i' target=\"_blank\">mild-yogurt-133</a></strong> to <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/4j01bn3i' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/4j01bn3i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 227 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | BatchNorm1d                  | 1.2 K  | train\n",
      "3 | predictor       | RankNetPredictor             | 90.6 K | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "319 K     Trainable params\n",
      "0         Non-trainable params\n",
      "319 K     Total params\n",
      "1.278     Total estimated model params size (MB)\n",
      "28        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c7198b854540af9e77a8dda8150fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21083d6a10714fc5b00c3fe1d3f68167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af481474d8f40cab2c950b94c8d6535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.558\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f01bc9b52db4be880cfbfd0eebf4657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56f4ec82e134324a64b203ae14713a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1410245f30844a79b47457a37a6007fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2539b6e96c53410e8896a2f639f23145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893ca250cbe04f8cb149c19357e3f3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08b3593553a4b149a8e60aa1698eba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14360c38aa0045d89c7a06661376ac1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c495b9226df47539c03baab0655bab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c7985f57cd408b8b621634bfc6c863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7d77f2788f4d84b3acbf002b82bbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"chemprop_delta_clf\", log_model=\"all\", save_code=True)\n",
    "wandb_logger.experiment.mark_preempting()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,  # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=50,  # number of epochs to train for\n",
    "    reload_dataloaders_every_n_epochs=1,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=10),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "contrastive_data_module = ConstrastiveDataModule(df_train, df_val)\n",
    "trainer.fit(mpnn, datamodule=contrastive_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "run_id = wandb_logger.experiment.id\n",
    "checkpoint_reference = f\"rahul-e-dev/chemprop_delta_clf/model-{run_id}:best\"\n",
    "artifact_dir = wandb_logger.download_artifact(checkpoint_reference, artifact_type=\"model\")\n",
    "\n",
    "\n",
    "ckpt = torch.load(Path(artifact_dir) / \"model.ckpt\", map_location='cpu', weights_only=False)\n",
    "hparams = ckpt.get('hyper_parameters', ckpt.get('hparams', {}))\n",
    "mpnn.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/rahul_e_dev/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50087fa936240a8bc3b3808b20c471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exemplar_df = df_train[df_train['per_inhibition'] >= 50].sample(100).reset_index(drop=True)\n",
    "\n",
    "exemplar_ds = ExemplarDataset(\n",
    "    df_test,\n",
    "    exemplar_df\n",
    ")\n",
    "\n",
    "exemplar_dl = DataLoader(\n",
    "    dataset=exemplar_ds,\n",
    "    batch_size=2048,\n",
    "    shuffle=False,\n",
    "    collate_fn=data.dataloader.collate_multicomponent,\n",
    "    num_workers=12,\n",
    ")\n",
    "\n",
    "test_ds_preds = trainer.predict(model=mpnn, dataloaders=exemplar_dl)\n",
    "test_ds_preds = torch.cat(test_ds_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def calc(x):\n",
    "    x = np.array(x)\n",
    "    return (x>=0.5).sum()\n",
    "\n",
    "\n",
    "deltas = defaultdict(list)\n",
    "for (i, j), delta in zip(exemplar_ds.pairs, test_ds_preds.squeeze()):\n",
    "    deltas[i].append(float(delta.item()))\n",
    "\n",
    "\n",
    "df_test['deltas'] = deltas\n",
    "df_test['pred_probs'] = df_test['deltas'].map(calc)\n",
    "df_test['means'] = df_test['deltas'].map(np.mean)\n",
    "df_test['std'] = df_test['deltas'].map(np.std)\n",
    "df_test['range'] = df_test['deltas'].map(lambda x: max(x) - min(x))\n",
    "df_test['preds'] = df_test['pred_probs'] >= 10\n",
    "df_test['true'] = df_test['per_inhibition'] >= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "wandb_logger.log_table(\n",
    "    'final_metrics', \n",
    "    ['f1', 'precision', 'recall', 'accuracy'],\n",
    "    [[\n",
    "        f1_score(df_test['true'], df_test['preds']),\n",
    "        precision_score(df_test['true'], df_test['preds']),\n",
    "        recall_score(df_test['true'], df_test['preds']),\n",
    "        accuracy_score(df_test['true'], df_test['preds'])\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88695fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▃▂▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▆▆▆▅▅▅▅▄▃▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>val/f1</td><td>▂▁▆▆▃▂██▆▇█</td></tr><tr><td>val/prc</td><td>▄▁▄▆▃▁▆▆▅▅█</td></tr><tr><td>val/roc</td><td>▄▁▅▆▃▁▇▇▅▆█</td></tr><tr><td>val_loss</td><td>▁▁▂▃▄▅▅▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss_epoch</td><td>0.12198</td></tr><tr><td>train_loss_step</td><td>0.11191</td></tr><tr><td>trainer/global_step</td><td>2232</td></tr><tr><td>val/f1</td><td>0.7388</td></tr><tr><td>val/prc</td><td>0.81523</td></tr><tr><td>val/roc</td><td>0.8142</td></tr><tr><td>val_loss</td><td>1.2989</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-spaceship-129</strong> at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/5zf8uqv5' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/5zf8uqv5</a><br> View project at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a><br>Synced 6 W&B file(s), 1 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251010_213654-5zf8uqv5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a61707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.402088772845953, 0.27208480565371024, 0.77, 0.6427457098283932]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[\n",
    "        f1_score(df_test['true'], df_test['preds']),\n",
    "        precision_score(df_test['true'], df_test['preds']),\n",
    "        recall_score(df_test['true'], df_test['preds']),\n",
    "        accuracy_score(df_test['true'], df_test['preds'])\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e6773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
