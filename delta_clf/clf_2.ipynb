{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befbca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/rahul_e_dev/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul-e-dev\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data, featurizers, models, nn\n",
    "from data import ConstrastiveDataModule, ExemplarDataset\n",
    "from dotenv import load_dotenv\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from commons.data import load_and_split_gsk_dataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "load_dotenv('.env.secret')\n",
    "wandb.login(key='cf344975eb80edf6f0d52af80528cc6094234caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c794f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_and_split_gsk_dataset(\"../GSK_HepG2.csv\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53dcdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdims = featurizers.SimpleMoleculeMolGraphFeaturizer().shape # the dimensions of the featurizer, given as (atom_dims, bond_dims).\n",
    "mcmp = nn.MulticomponentMessagePassing(\n",
    "    blocks=[nn.BondMessagePassing(*fdims)],\n",
    "    n_components=2,\n",
    "    shared=True\n",
    ")\n",
    "agg = nn.NormAggregation()\n",
    "ffn = nn.BinaryClassificationFFN(n_tasks=1, input_dim=mcmp.output_dim)\n",
    "batch_norm = True\n",
    "metric_list = [nn.metrics.BinaryF1Score(), nn.metrics.BinaryAUPRC(), nn.metrics.BinaryAUROC()]\n",
    "mpnn = models.multi.MulticomponentMPNN(mcmp, agg, ffn, batch_norm, metric_list)\n",
    "mpnn.max_lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151275e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250726_165223-jb5upe17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/jb5upe17' target=\"_blank\">helpful-dew-79</a></strong> to <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/jb5upe17' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/jb5upe17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 227 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | BatchNorm1d                  | 1.2 K  | train\n",
      "3 | predictor       | BinaryClassificationFFN      | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "409 K     Trainable params\n",
      "0         Non-trainable params\n",
      "409 K     Total params\n",
      "1.638     Total estimated model params size (MB)\n",
      "28        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529a5265526a49558918d1ec478ae8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6673f797f2b141039d66724840b45617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865fbb9dc0c04fc191c3f713003f1450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e219c834d3ad4d5a88e26acdcf7412d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d076813408714fa29f0e142a8d132a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14561bd4c7f94592af86bed1a0613b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67af9a18ce9486ea6f3314414b5f115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f08346b63204447ab78a732dc3da37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.588. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"chemprop_delta_clf\", log_model=\"all\", save_code=True)\n",
    "wandb_logger.experiment.mark_preempting()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,  # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=50,  # number of epochs to train for\n",
    "    reload_dataloaders_every_n_epochs=1,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=5),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "contrastive_data_module = ConstrastiveDataModule(df_train, df_val)\n",
    "trainer.fit(mpnn, datamodule=contrastive_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "run_id = wandb_logger.experiment.id\n",
    "checkpoint_reference = f\"rahul-e-dev/chemprop_delta_clf/model-{run_id}:best\"\n",
    "artifact_dir = wandb_logger.download_artifact(checkpoint_reference, artifact_type=\"model\")\n",
    "\n",
    "\n",
    "ckpt = torch.load(Path(artifact_dir) / \"model.ckpt\", map_location='cpu', weights_only=False)\n",
    "hparams = ckpt.get('hyper_parameters', ckpt.get('hparams', {}))\n",
    "mpnn.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb238d26a7b44b492f9114a07f8b7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exemplar_df = df_train[df_train['per_inhibition'] >= -20].sample(100).reset_index(drop=True)\n",
    "\n",
    "exemplar_ds = ExemplarDataset(\n",
    "    df_test,\n",
    "    exemplar_df\n",
    ")\n",
    "\n",
    "exemplar_dl = DataLoader(\n",
    "    dataset=exemplar_ds,\n",
    "    batch_size=2048,\n",
    "    shuffle=False,\n",
    "    collate_fn=data.dataloader.collate_multicomponent,\n",
    "    num_workers=12,\n",
    ")\n",
    "\n",
    "test_ds_preds = trainer.predict(model=mpnn, dataloaders=exemplar_dl)\n",
    "test_ds_preds = torch.cat(test_ds_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fd1b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def calc(x):\n",
    "    x = np.array(x)\n",
    "    return (x>=0.5).sum()\n",
    "\n",
    "\n",
    "deltas = defaultdict(list)\n",
    "for (i, j), delta in zip(exemplar_ds.pairs, test_ds_preds.squeeze()):\n",
    "    exemplar_val = exemplar_ds.df_exemplars['per_inhibition'][j]\n",
    "    deltas[i].append(float(delta.item()))\n",
    "\n",
    "\n",
    "df_test['deltas'] = deltas\n",
    "df_test['pred_probs'] = df_test['deltas'].map(calc)\n",
    "df_test['means'] = df_test['deltas'].map(np.mean)\n",
    "df_test['std'] = df_test['deltas'].map(np.std)\n",
    "df_test['range'] = df_test['deltas'].map(lambda x: max(x) - min(x))\n",
    "df_test['preds'] = df_test['pred_probs'] >= 50\n",
    "df_test['true'] = df_test['per_inhibition'] >= -15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbc4c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "wandb_logger.log_table(\n",
    "    'final_metrics', \n",
    "    ['f1', 'precision', 'recall', 'accuracy'],\n",
    "    [[\n",
    "        f1_score(df_test['true'], df_test['preds']),\n",
    "        precision_score(df_test['true'], df_test['preds']),\n",
    "        recall_score(df_test['true'], df_test['preds']),\n",
    "        accuracy_score(df_test['true'], df_test['preds'])\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88695fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▄▄▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▆▆▆▆▅▆▅▅▅▄▄▄▄▄▄▃▄▃▃▃▃▂▃▂▃▃▂▂▂▂▂▁▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>val/f1</td><td>█▁▃▄▅▃</td></tr><tr><td>val/prc</td><td>█▂▂▂▁▁</td></tr><tr><td>val/roc</td><td>█▃▃▃▄▁</td></tr><tr><td>val_loss</td><td>▁▂▃▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss_epoch</td><td>0.28002</td></tr><tr><td>train_loss_step</td><td>0.28673</td></tr><tr><td>trainer/global_step</td><td>2429</td></tr><tr><td>val/f1</td><td>0.70891</td></tr><tr><td>val/prc</td><td>0.77652</td></tr><tr><td>val/roc</td><td>0.7765</td></tr><tr><td>val_loss</td><td>0.80897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-dew-79</strong> at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/jb5upe17' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/jb5upe17</a><br> View project at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a><br>Synced 8 W&B file(s), 1 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250726_165223-jb5upe17/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a61707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
