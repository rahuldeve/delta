{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data, featurizers, models, nn\n",
    "# from data import ConstrastiveDataModule, ExemplarDataset\n",
    "from dotenv import load_dotenv\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "\n",
    "import wandb\n",
    "# from commons.data import load_and_split_gsk_dataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "load_dotenv('.env.secret')\n",
    "wandb.login(key='cf344975eb80edf6f0d52af80528cc6094234caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from commons.utils import get_scaffold, standardize\n",
    "\n",
    "\n",
    "def mol_to_inchi(mol):\n",
    "    with BlockLogs():\n",
    "        return Chem.MolToInchi(mol)\n",
    "\n",
    "\n",
    "def load_and_split_gsk_dataset(path, RANDOM_SEED):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[:, 1:]\n",
    "    df.columns = [\"smiles\", \"per_inhibition\"]\n",
    "    df[\"per_inhibition\"] = 100 - (df[\"per_inhibition\"].clip(upper=100))\n",
    "\n",
    "    # standardize and convert to inchi\n",
    "    df[\"mol\"] = df[\"smiles\"].map(standardize)\n",
    "    df = df.dropna(subset=[\"mol\"])\n",
    "    df[\"inchi\"] = df[\"mol\"].map(mol_to_inchi)\n",
    "    df = df.groupby([\"inchi\"]).filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "\n",
    "    clusters, _ = pd.factorize(\n",
    "        df[\"mol\"]\n",
    "        .map(Chem.MolToSmiles)  # type: ignore\n",
    "        .map(get_scaffold)\n",
    "    )\n",
    "    clusters = pd.Series(clusters)\n",
    "\n",
    "    df = df.drop([\"smiles\", \"inchi\"], axis=1)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED)\n",
    "    train_idxs, val_test_idxs = next(splitter.split(df, groups=clusters))\n",
    "    df_train = df.loc[train_idxs].reset_index(drop=True)\n",
    "    df_val_test = df.loc[val_test_idxs].reset_index(drop=True)\n",
    "    clusters_val_test = clusters.iloc[val_test_idxs].reset_index(drop=True)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED, test_size=0.5)\n",
    "    val_idxs, test_idxs = next(splitter.split(df_val_test, groups=clusters_val_test))\n",
    "    df_val = df_val_test.loc[val_idxs].reset_index(drop=True)\n",
    "    df_test = df_val_test.loc[test_idxs].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c794f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_and_split_gsk_dataset(\"../GSK_HepG2.csv\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea35eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_hard_and_negatives(\n",
    "    all_embeds: torch.Tensor,\n",
    "    all_targets: torch.Tensor,\n",
    "    n_candidates=31,\n",
    "    n_hard=8\n",
    "):\n",
    "    # print(type(all_embeds))\n",
    "    device = all_embeds.device\n",
    "    B = all_embeds.shape[0]\n",
    "    n_rand = n_candidates - n_hard\n",
    "\n",
    "    logits = all_embeds @ all_embeds.T\n",
    "\n",
    "    target_closness_mask = (all_targets.view(-1, 1) - all_targets).abs() <= W\n",
    "    # assign -inf to any values entries present in target_closness_mask\n",
    "    logits[target_closness_mask] = float(\"-inf\")\n",
    "\n",
    "    _, hard_candidate_idxs = torch.topk(\n",
    "        logits, k=n_hard, dim=1, largest=True, sorted=False\n",
    "    )\n",
    "\n",
    "    # generate a probability matrix where we assign 0 probability to entries either present in\n",
    "    # target_closness_mask or they were selected as hard negatives\n",
    "    rand_candidates_selection = torch.where(~target_closness_mask, 1.0, float(\"-inf\"))\n",
    "    row_idxs = torch.arange(B, device=device).unsqueeze(1).expand(-1, n_hard)\n",
    "    # assign any hard negative candidates with 0 probability\n",
    "    rand_candidates_selection[row_idxs, hard_candidate_idxs] = float(\"-inf\")\n",
    "    rand_candidates_selection_prob = rand_candidates_selection.softmax(dim=-1)\n",
    "    rand_candidate_idxs = torch.multinomial(\n",
    "        rand_candidates_selection_prob, n_rand, replacement=False\n",
    "    )\n",
    "\n",
    "    candidate_idxs = torch.cat([hard_candidate_idxs, rand_candidate_idxs], dim=-1)\n",
    "    assert candidate_idxs.shape == (logits.shape[0], n_candidates)\n",
    "    return candidate_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e44506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import NamedTuple\n",
    "from chemprop.data.datasets import Datum\n",
    "from chemprop.data.collate import TrainingBatch, collate_batch\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class ContrastiveDataPoint(NamedTuple):\n",
    "    anchor: Datum\n",
    "    candidates: list[Datum]\n",
    "\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, mols, targets, n_candidates=31, n_hard=2):\n",
    "        self.mols = mols\n",
    "        self.targets = targets\n",
    "        self.featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "        self.n_candidates = n_candidates\n",
    "        self.n_hard = n_hard\n",
    "        self.n_random = n_candidates - n_hard\n",
    "\n",
    "    def build_init_candidates(self):\n",
    "        # build initial candidates as random\n",
    "        selection_mat = torch.ones(len(self.targets), len(self.targets))\n",
    "        selection_mat.fill_diagonal_(float(\"-inf\"))\n",
    "        selection_prob = selection_mat.softmax(dim=-1)\n",
    "        rand_idxs = torch.multinomial(\n",
    "            selection_prob, self.n_candidates, replacement=False\n",
    "        )\n",
    "        self.candidates = rand_idxs.numpy()\n",
    "\n",
    "    def get_pos_candidate_idx(self, idx):\n",
    "        anchor_target = self.targets[idx]\n",
    "        mask = (self.targets - anchor_target).abs() <= W\n",
    "        return int(self.targets[mask].sample(1).index[0])\n",
    "\n",
    "    def get_datum(self, idx):\n",
    "        mg = self.featurizer(self.mols[idx])\n",
    "        target = self.targets[[idx]].to_numpy()\n",
    "        return data.datasets.Datum(mg, None, None, target, 1.0, None, None)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return ContrastiveDataPoint(\n",
    "            self.get_datum(idx),\n",
    "            [self.get_datum(int(c_idx)) for c_idx in self.candidates[idx]],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mols)\n",
    "\n",
    "\n",
    "class ContrastiveTrainingBatch(NamedTuple):\n",
    "    anchor: TrainingBatch\n",
    "    candidates: TrainingBatch\n",
    "    B: int\n",
    "    C: int\n",
    "\n",
    "\n",
    "def collate_contrastive(batch):\n",
    "    batch_anchors, batch_candidates = zip(*batch)\n",
    "    B = len(batch)\n",
    "    C = len(batch_candidates[0])\n",
    "    batch_anchors = collate_batch(batch_anchors)\n",
    "    batch_candidates = collate_batch(chain.from_iterable(batch_candidates))\n",
    "    return ContrastiveTrainingBatch(\n",
    "        batch_anchors, batch_candidates, B=B, C=C\n",
    "    )\n",
    "\n",
    "\n",
    "class SimpleDataPoint(NamedTuple):\n",
    "    anchor: Datum\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, mols, targets):\n",
    "        self.mols = mols\n",
    "        self.targets = targets\n",
    "        self.featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "    def get_datum(self, idx):\n",
    "        mg = self.featurizer(self.mols[idx])\n",
    "        target = self.targets[[idx]].to_numpy()\n",
    "        return data.datasets.Datum(mg, None, None, target, 1.0, None, None)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_datum(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mols)\n",
    "    \n",
    "\n",
    "# class SimpleBatch(NamedTuple):\n",
    "#     anchor: TrainingBatch\n",
    "\n",
    "\n",
    "# def collate_simple(batch):\n",
    "#     batch_anchors = collate_batch(batch)\n",
    "#     return SimpleBatch(batch_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36472972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrastiveDataModule(L.LightningDataModule):\n",
    "    def __init__(self, df_train, df_val) -> None:\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "        self.batch_size=32\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_ds = ContrastiveDataset(\n",
    "            self.df_train[\"mol\"], \n",
    "            self.df_train[\"per_inhibition\"]\n",
    "        )\n",
    "        self.val_ds = ContrastiveDataset(\n",
    "            self.df_val[\"mol\"], \n",
    "            self.df_val[\"per_inhibition\"]\n",
    "        )\n",
    "\n",
    "        self.train_ds.build_init_candidates()\n",
    "        self.val_ds.build_init_candidates()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_contrastive,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_contrastive,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def update_train_dataset_neg_candidates(self, candidate_idxs: torch.Tensor):\n",
    "        assert candidate_idxs.shape[0] == self.df_train.shape[0]\n",
    "        self.train_ds.candidates = candidate_idxs.cpu().numpy()\n",
    "\n",
    "    def update_val_dataset_neg_candidates(self, candidate_idxs: torch.Tensor):\n",
    "        assert candidate_idxs.shape[0] == self.df_val.shape[0], (candidate_idxs.shape, self.df_val.shape)\n",
    "        self.val_ds.candidates = candidate_idxs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10435d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable\n",
    "from chemprop.nn import Aggregation, ChempropMetric, MessagePassing, Predictor\n",
    "from chemprop.nn.transforms import ScaleTransform\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class ContrastiveMPNN(models.MPNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: MessagePassing,\n",
    "        agg: Aggregation,\n",
    "        predictor: Predictor,\n",
    "        batch_norm: bool = False,\n",
    "        metrics: Iterable[ChempropMetric] | None = None,\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 0.0001,\n",
    "        max_lr: float = 0.001,\n",
    "        final_lr: float = 0.0001,\n",
    "        X_d_transform: ScaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message_passing,\n",
    "            agg,\n",
    "            predictor,\n",
    "            batch_norm,\n",
    "            metrics,\n",
    "            warmup_epochs,\n",
    "            init_lr,\n",
    "            max_lr,\n",
    "            final_lr,\n",
    "            X_d_transform,\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def embed_simple_batch(self, batch: TrainingBatch):\n",
    "        bmg, V_d, X_d, target, _, _, _ = batch\n",
    "        Z_anchor = self.encoding(bmg, V_d, X_d)\n",
    "        return dict(embeds=Z_anchor, targets=target)\n",
    "\n",
    "\n",
    "    def training_step(self, batch: ContrastiveTrainingBatch, batch_idx):  # type: ignore\n",
    "        B, C = batch.B, batch.C\n",
    "\n",
    "        bmg, V_d, X_d, target_anchor, _, _, _ = batch.anchor\n",
    "        Z_anchor = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        bmg, V_d, X_d, target_candidates, _, _, _ = batch.candidates\n",
    "        Z_candidates = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        Z_anchor = Z_anchor.view((B, 1, -1))  # (B X 1 X d)\n",
    "        Z_candidates = Z_candidates.view((B, C, -1))  # (B X d X C)\n",
    "        # (B X 1 X d)  x  (B X d X C) --> B X 1 X C --> B X C\n",
    "        logits = (Z_anchor @ Z_candidates.transpose(1, 2)).view(B, -1)\n",
    "        labels = torch.zeros(B).long().to(self.device)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, batch_size=B, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: ContrastiveTrainingBatch, batch_idx):  # type: ignore\n",
    "        B, C = batch.B, batch.C\n",
    "\n",
    "        bmg, V_d, X_d, target_anchor, _, _, _ = batch.anchor\n",
    "        Z_anchor = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        bmg, V_d, X_d, target_candidates, _, _, _ = batch.candidates\n",
    "        Z_candidates = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        Z_anchor = Z_anchor.view((B, 1, -1))  # (B X 1 X d)\n",
    "        Z_candidates = Z_candidates.view((B, C, -1))  # (B X d X C)\n",
    "        # (B X 1 X d)  x  (B X d X C) --> B X 1 X C --> B X C\n",
    "        logits = (Z_anchor @ Z_candidates.transpose(1, 2)).view(B, -1)\n",
    "        labels = torch.zeros(B).long().to(self.device)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss, batch_size=B)\n",
    "        return loss\n",
    "\n",
    "    def get_candidates(self, dl: DataLoader, stage_str: str):\n",
    "        all_embeds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dl, desc=f\"Mining {stage_str}:\", leave=False):\n",
    "                batch = move_data_to_device(batch, \"cuda\")\n",
    "                res = self.trainer.model.embed_simple_batch(batch)  # type: ignore\n",
    "\n",
    "                all_embeds.append(res[\"embeds\"])\n",
    "                all_targets.append(res[\"targets\"])\n",
    "\n",
    "        all_embeds = torch.cat(all_embeds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        return mine_hard_and_negatives(all_embeds.squeeze(), all_targets.squeeze())\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        ds = SimpleDataset(\n",
    "            self.trainer.datamodule.df_train[\"mol\"],  # type: ignore\n",
    "            self.trainer.datamodule.df_train[\"per_inhibition\"],  # type: ignore\n",
    "        )\n",
    "        dl = DataLoader(\n",
    "            ds,\n",
    "            collate_fn=collate_batch,\n",
    "            batch_size=self.trainer.datamodule.batch_size, # type: ignore\n",
    "            shuffle=False,\n",
    "        )\n",
    "        neg_candidate_idxs = self.get_candidates(dl, \"Train\")\n",
    "        self.trainer.datamodule.update_train_dataset_neg_candidates(  # type: ignore\n",
    "            neg_candidate_idxs.to(\"cpu\")\n",
    "        )\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        ds = SimpleDataset(\n",
    "            self.trainer.datamodule.df_val[\"mol\"],  # type: ignore\n",
    "            self.trainer.datamodule.df_val[\"per_inhibition\"],  # type: ignore\n",
    "        )\n",
    "        dl = DataLoader(\n",
    "            ds,\n",
    "            collate_fn=collate_batch,\n",
    "            batch_size=self.trainer.datamodule.batch_size, # type: ignore\n",
    "            shuffle=False,\n",
    "        )\n",
    "        neg_candidate_idxs = self.get_candidates(dl, \"Val\")\n",
    "        self.trainer.datamodule.update_val_dataset_neg_candidates(  # type: ignore\n",
    "            neg_candidate_idxs.to(\"cpu\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdims = featurizers.SimpleMoleculeMolGraphFeaturizer().shape # the dimensions of the featurizer, given as (atom_dims, bond_dims).\n",
    "mp = nn.BondMessagePassing()\n",
    "agg = nn.NormAggregation()\n",
    "ffn = nn.BinaryClassificationFFN(n_tasks=1)\n",
    "batch_norm = True\n",
    "metric_list = [nn.metrics.BinaryF1Score(), nn.metrics.BinaryAUPRC(), nn.metrics.BinaryAUROC()]\n",
    "contrastive_mpnn = ContrastiveMPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "# contrastive_mpnn.max_lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"chemprop_delta_clf\", log_model=\"all\", save_code=True)\n",
    "wandb_logger.experiment.mark_preempting()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,  # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=50,  # number of epochs to train for\n",
    "    reload_dataloaders_every_n_epochs=1,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=10),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(contrastive_mpnn, datamodule=ConstrastiveDataModule(df_train, df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa2714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298728cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ad9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43148f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
