{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e763a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data, featurizers, models\n",
    "from chemprop import nn as chem_nn\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "import pandas as pd\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem.Descriptors import CalcMolDescriptors\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from commons.utils import get_scaffold, standardize\n",
    "from typing import NamedTuple\n",
    "from itertools import chain\n",
    "\n",
    "import wandb\n",
    "# from commons.data import load_and_split_gsk_dataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "# load_dotenv('.env.secret')\n",
    "# wandb.login(key='cf344975eb80edf6f0d52af80528cc6094234caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb18a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_inchi(mol):\n",
    "    with BlockLogs():\n",
    "        return Chem.MolToInchi(mol)\n",
    "    \n",
    "\n",
    "def generate_features(df):\n",
    "    with BlockLogs():\n",
    "        feats = pd.DataFrame.from_records(df[\"mol\"].map(CalcMolDescriptors).tolist())\n",
    "        feats.columns = [f\"feat_{f}\" for f in feats.columns]\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df.reset_index(drop=True),\n",
    "                feats,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_split_gsk_dataset(path, RANDOM_SEED):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[:, 1:]\n",
    "    df.columns = [\"smiles\", \"per_inhibition\"]\n",
    "\n",
    "    # standardize and convert to inchi\n",
    "    df[\"mol\"] = df[\"smiles\"].map(standardize)\n",
    "    df = df.dropna(subset=[\"mol\"])\n",
    "    df[\"inchi\"] = df[\"mol\"].map(mol_to_inchi)\n",
    "    df = df.groupby([\"inchi\"]).filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "\n",
    "    df[\"is_cytotoxic\"] = df[\"per_inhibition\"] > 50.0\n",
    "\n",
    "    df = generate_features(df)\n",
    "\n",
    "    clusters, _ = pd.factorize(\n",
    "        df[\"mol\"]\n",
    "        .map(Chem.MolToSmiles)  # type: ignore\n",
    "        .map(get_scaffold)\n",
    "    )\n",
    "    clusters = pd.Series(clusters)\n",
    "\n",
    "    df = df.drop([\"smiles\", \"inchi\"], axis=1)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED)\n",
    "    train_idxs, val_test_idxs = next(splitter.split(df, groups=clusters))\n",
    "    df_train = df.loc[train_idxs].reset_index(drop=True)\n",
    "    df_val_test = df.loc[val_test_idxs].reset_index(drop=True)\n",
    "    clusters_val_test = clusters.iloc[val_test_idxs].reset_index(drop=True)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED, test_size=0.5)\n",
    "    val_idxs, test_idxs = next(splitter.split(df_val_test, groups=clusters_val_test))\n",
    "    df_val = df_val_test.loc[val_idxs].reset_index(drop=True)\n",
    "    df_test = df_val_test.loc[test_idxs].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0aecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_and_split_gsk_dataset(\"../GSK_HepG2.csv\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1974a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_datapoint(row):\n",
    "    feat_entry_names = [f for f in row.index if f.startswith('feat')]\n",
    "    feat_array = pd.to_numeric(row[feat_entry_names], errors=\"coerce\")\n",
    "    return data.MoleculeDatapoint(\n",
    "        mol=row['mol'], \n",
    "        y=np.array([row['per_inhibition']]),\n",
    "        x_d=feat_array.to_numpy()\n",
    "    )\n",
    "\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "train_mol_dataset = data.MoleculeDataset(df_train.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "val_mol_dataset = data.MoleculeDataset(df_val.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "test_mol_dataset = data.MoleculeDataset(df_test.apply(get_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "\n",
    "x_d_scaler = train_mol_dataset.normalize_inputs(\"X_d\")\n",
    "val_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "test_mol_dataset.normalize_inputs(\"X_d\", x_d_scaler)\n",
    "\n",
    "train_mol_dataset.cache = True\n",
    "val_mol_dataset.cache = True\n",
    "test_mol_dataset.cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa43d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPairDataPoint(NamedTuple):\n",
    "    anchor: data.datasets.Datum\n",
    "    exemplar: list[data.datasets.Datum]\n",
    "    random: list[data.datasets.Datum]\n",
    "\n",
    "\n",
    "class RandomPairTrainBatch(NamedTuple):\n",
    "    anchor: data.collate.TrainingBatch\n",
    "    exemplar: data.collate.TrainingBatch\n",
    "    random: data.collate.TrainingBatch\n",
    "    B: int\n",
    "    C: int\n",
    "    \n",
    "\n",
    "class RandomPairDataset(Dataset):\n",
    "    def __init__(self, mol_dataset, n_candidates):\n",
    "        super().__init__()\n",
    "        self.mol_dataset: data.datasets.MoleculeDataset = mol_dataset\n",
    "        self.n_candidates: int = n_candidates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mol_dataset)\n",
    "    \n",
    "    def get_exemplar_candidates(self):\n",
    "        targets = self.mol_dataset.Y.squeeze()\n",
    "        mask = targets > 50\n",
    "        weights = np.where(mask, 1.0, 0.0)\n",
    "        probs = weights / weights.sum()\n",
    "        exemplar_idxs = np.random.choice(\n",
    "            targets.shape[0], \n",
    "            size=(self.n_candidates,), \n",
    "            p=probs, \n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        return [self.mol_dataset[idx] for idx in exemplar_idxs]\n",
    "\n",
    "    \n",
    "    def get_random_candidates(self):\n",
    "        targets = self.mol_dataset.Y.squeeze()\n",
    "        candidate_idxs = np.random.choice(\n",
    "            targets.shape[0], \n",
    "            size=(self.n_candidates,), \n",
    "            replace=False\n",
    "        )\n",
    "        return [self.mol_dataset[idx] for idx in candidate_idxs]\n",
    "\n",
    "    def __getitem__(self, idx) -> RandomPairDataPoint:\n",
    "        return RandomPairDataPoint(\n",
    "            self.mol_dataset[idx], \n",
    "            self.get_exemplar_candidates(),\n",
    "            self.get_random_candidates()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_function(batch):\n",
    "        batch_anchors, batch_exemplars, batch_candidates = zip(*batch)\n",
    "        B = len(batch)\n",
    "        C = len(batch_candidates[0])\n",
    "        batch_anchors = data.dataloader.collate_batch(batch_anchors)\n",
    "        batch_exemplars = data.dataloader.collate_batch(chain.from_iterable(batch_exemplars))\n",
    "        batch_candidates = data.dataloader.collate_batch(chain.from_iterable(batch_candidates))\n",
    "        return RandomPairTrainBatch(batch_anchors, batch_exemplars, batch_candidates, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe77114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class RandomPairDataModule(L.LightningDataModule):\n",
    "    def __init__(self, mol_ds_train, mol_ds_val) -> None:\n",
    "        super().__init__()\n",
    "        self.mol_ds_train: data.MoleculeDataset = mol_ds_train\n",
    "        self.mol_ds_val: data.MoleculeDataset = mol_ds_val\n",
    "        self.batch_size=32\n",
    "        self.candidate_size=8\n",
    "\n",
    "        self.ds_train = None\n",
    "        self.ds_val = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds_train = RandomPairDataset(self.mol_ds_train, self.candidate_size)\n",
    "        self.ds_val = RandomPairDataset(self.mol_ds_val, self.candidate_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        assert self.ds_train is not None\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=RandomPairDataset.collate_function,\n",
    "            worker_init_fn=seed_worker,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        assert self.ds_val is not None\n",
    "        return DataLoader(\n",
    "            self.ds_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=RandomPairDataset.collate_function,\n",
    "            num_workers=8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22de2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable\n",
    "from chemprop.nn import Aggregation, ChempropMetric, MessagePassing, Predictor\n",
    "from chemprop.nn.transforms import ScaleTransform\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class RESCALInteraction(torch.nn.Module):\n",
    "    def __init__(self, ndims) -> None:\n",
    "        super().__init__()\n",
    "        self.interaction_matrix = torch.nn.Linear(ndims, ndims, bias=False)\n",
    "        self.head_dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, head_emb, tail_emb):\n",
    "        R = self.interaction_matrix.weight.unsqueeze(0)\n",
    "        z = self.head_dropout(head_emb @ R) @ tail_emb.transpose(-2, -1)\n",
    "        return z.squeeze().abs()\n",
    "    \n",
    "\n",
    "class ProjEInteraction(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embs = torch.nn.Embedding(4, 300)\n",
    "        self.dh_emb_idx = torch.nn.Parameter(torch.LongTensor([0]), requires_grad=False)\n",
    "        self.dr_emb_idx = torch.nn.Parameter(torch.LongTensor([1]), requires_grad=False)\n",
    "        self.b_emb_idx = torch.nn.Parameter(torch.LongTensor([2]), requires_grad=False)\n",
    "        self.r_emb_idx = torch.nn.Parameter(torch.LongTensor([3]), requires_grad=False)\n",
    "        self.b_p = torch.nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, head_emb, tail_emb):\n",
    "        dh_emb = self.embs(self.dh_emb_idx)\n",
    "        dr_emb = self.embs(self.dr_emb_idx)\n",
    "        r_emb = self.embs(self.r_emb_idx)\n",
    "        b_emb = self.embs(self.b_emb_idx)\n",
    "        b_p = self.b_p\n",
    "\n",
    "        x = torch.nn.functional.relu(\n",
    "            dh_emb*head_emb + dr_emb*r_emb + b_emb\n",
    "        )\n",
    "        y = x@tail_emb.transpose(-2, -1) + b_p\n",
    "        return y.squeeze()\n",
    "        \n",
    "\n",
    "class NNInteraction(torch.nn.Module):\n",
    "    def __init__(self, ndims) -> None:\n",
    "        super().__init__()\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2* ndims, 2* ndims),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2* ndims, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, head_emb, tail_emb):\n",
    "        if head_emb.shape[-2] == 1:\n",
    "            head_emb = head_emb.expand_as(tail_emb)\n",
    "        else:\n",
    "            tail_emb = tail_emb.expand_as(head_emb)\n",
    "            \n",
    "        Z_combined = torch.cat([head_emb, tail_emb], dim=-1)    # (B, C, 2*d)\n",
    "        return self.nn(Z_combined).squeeze()\n",
    "\n",
    "\n",
    "class ContrastiveMPNN(models.MPNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: MessagePassing,\n",
    "        agg: Aggregation,\n",
    "        predictor: Predictor,\n",
    "        batch_norm: bool = False,\n",
    "        metrics: Iterable[ChempropMetric] | None = None,\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 0.0001,\n",
    "        max_lr: float = 0.001,\n",
    "        final_lr: float = 0.0001,\n",
    "        X_d_transform: ScaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message_passing,\n",
    "            agg,\n",
    "            predictor,\n",
    "            batch_norm,\n",
    "            metrics,\n",
    "            warmup_epochs,\n",
    "            init_lr,\n",
    "            max_lr,\n",
    "            final_lr,\n",
    "            X_d_transform,\n",
    "        )\n",
    "\n",
    "        self.interaction = RESCALInteraction(300)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    def embed_simple_batch(self, batch: data.collate.TrainingBatch):\n",
    "        bmg, V_d, X_d, target, _, _, _ = batch\n",
    "        Z = self.encoding(bmg, V_d, X_d)\n",
    "        return dict(embeds=Z, targets=target)\n",
    "    \n",
    "\n",
    "    def get_losses(self, batch: RandomPairTrainBatch):\n",
    "        B, C = batch.B, batch.C\n",
    "\n",
    "        bmg, V_d, X_d, target_anchor, _, _, _ = batch.anchor\n",
    "        Z_anchor = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        bmg, V_d, X_d, target_exemplar, _, _, _ = batch.exemplar\n",
    "        Z_exemplar = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        bmg, V_d, X_d, target_random, _, _, _ = batch.random\n",
    "        Z_random = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        Z_anchor = Z_anchor.view(B, 1, -1)              # (B, d) -> (B, 1, d)\n",
    "        Z_exemplar = Z_exemplar.view(B, C, -1)          # (B*C, d) -> (B, C, d)\n",
    "        Z_random = Z_random.view(B, C, -1)              # (B*C, d) -> (B, C, d)\n",
    "        \n",
    "        target_anchor = target_anchor.view(-1, 1)\n",
    "        target_exemplar = target_exemplar.view(B, C)\n",
    "        target_random = target_random.view(B, C)\n",
    "\n",
    "\n",
    "        # left to right loss\n",
    "        preds = self.interaction(Z_anchor, Z_exemplar).squeeze()\n",
    "        labels = (target_anchor > target_exemplar).float() # type: ignore\n",
    "        lr_exemplar_loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        preds = self.interaction(Z_anchor, Z_random).squeeze()\n",
    "        labels = (target_anchor > target_random).float() # type: ignore\n",
    "        lr_random_loss = self.loss_fn(preds, labels)\n",
    "       \n",
    "\n",
    "        # right to left loss\n",
    "        preds = self.interaction(Z_exemplar, Z_anchor).squeeze()\n",
    "        labels = (target_exemplar > target_anchor).float() # type: ignore\n",
    "        rl_exemplar_loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        atleast_one = torch.exp(1 - torch.nn.functional.logsigmoid(preds).sum(axis=-1))\n",
    "        labels = (target_anchor > 50).float()\n",
    "        asd_loss = torch.nn.functional.binary_cross_entropy(atleast_one.unsqueeze(-1), labels)\n",
    "\n",
    "        preds = self.interaction(Z_random, Z_anchor).squeeze()\n",
    "        labels = (target_random > target_anchor).float() # type: ignore\n",
    "        rl_random_loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        loss = (lr_exemplar_loss + lr_random_loss + rl_exemplar_loss + rl_random_loss) / 4\n",
    "        return loss + asd_loss, (lr_exemplar_loss, lr_random_loss, rl_exemplar_loss, rl_random_loss)\n",
    "\n",
    "\n",
    "    def training_step(self, batch: RandomPairTrainBatch, batch_idx):  # type: ignore\n",
    "        loss, (lr_exemplar_loss, lr_random_loss, rl_exemplar_loss, rl_random_loss) = self.get_losses(batch)\n",
    "        self.log(\"train_lr_exemplar_loss\", lr_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_lr_random_loss\", lr_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_rl_exemplar_loss\", rl_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_rl_random_loss\", rl_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"train_loss\", loss, batch_size=batch.B, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: RandomPairTrainBatch, batch_idx):  # type: ignore\n",
    "        loss, (lr_exemplar_loss, lr_random_loss, rl_exemplar_loss, rl_random_loss) = self.get_losses(batch)\n",
    "        self.log(\"val_lr_exemplar_loss\", lr_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_lr_random_loss\", lr_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_rl_exemplar_loss\", rl_exemplar_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_rl_random_loss\", rl_random_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "        self.log(\"val_loss\", loss, batch_size=batch.B)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8594915",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdims = featurizers.SimpleMoleculeMolGraphFeaturizer().shape # the dimensions of the featurizer, given as (atom_dims, bond_dims).\n",
    "mp = chem_nn.BondMessagePassing()\n",
    "agg = chem_nn.NormAggregation()\n",
    "ffn_dims = mp.output_dim + len([f for f in df_train.columns if f.startswith(\"feat\")])\n",
    "ffn = chem_nn.BinaryClassificationFFN(n_tasks=1, input_dim=ffn_dims, activation=torch.nn.ELU(), dropout=0.3)\n",
    "batch_norm = True\n",
    "metric_list = [chem_nn.metrics.BinaryF1Score(), chem_nn.metrics.BinaryAUPRC(), chem_nn.metrics.BinaryAUROC()]\n",
    "X_d_transform = chem_nn.ScaleTransform.from_standard_scaler(x_d_scaler)\n",
    "contrastive_mpnn = ContrastiveMPNN(mp, agg, ffn, batch_norm, metric_list, X_d_transform=X_d_transform)\n",
    "# contrastive_mpnn.max_lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e0455da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-cloud-220</strong> at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/6ujdy2uy' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/6ujdy2uy</a><br> View project at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a><br>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251019_223628-6ujdy2uy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20251019_224014-zvn0y2fl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/zvn0y2fl' target=\"_blank\">valiant-feather-221</a></strong> to <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/zvn0y2fl' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/zvn0y2fl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:45: Attribute 'metrics' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['metrics'])`.\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:45: Attribute 'X_d_transform' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['X_d_transform'])`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type                    | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing      | 227 K  | train\n",
      "1 | agg             | NormAggregation         | 0      | train\n",
      "2 | bn              | BatchNorm1d             | 600    | train\n",
      "3 | predictor       | BinaryClassificationFFN | 155 K  | train\n",
      "4 | X_d_transform   | ScaleTransform          | 0      | train\n",
      "5 | metrics         | ModuleList              | 0      | train\n",
      "6 | interaction     | RESCALInteraction       | 90.0 K | train\n",
      "7 | loss_fn         | BCEWithLogitsLoss       | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "474 K     Trainable params\n",
      "0         Non-trainable params\n",
      "474 K     Total params\n",
      "1.896     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17513ebd5d23451e951ea69991bbf301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [11,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [17,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [20,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [21,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [22,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [23,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [25,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [26,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [27,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [29,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/chemprop/data/collate.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"A :class:`BatchMolGraph` represents a batch of individual :class:`MolGraph`\\s.\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/chemprop/data/collate.py:21: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"A list of individual :class:`MolGraph`\\s to be batched together\"\"\"\n",
      "/home/rahul/delta/.venv/lib/python3.12/site-packages/chemprop/data/collate.py:64: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"the number of individual :class:`MolGraph`\\s in this batch\"\"\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:411\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    410\u001b[39m     batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     batch = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_to_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# the `_step` methods don't take a batch_idx when `dataloader_iter` is used, but all other hooks still do,\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# so we need different kwargs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:278\u001b[39m, in \u001b[36mStrategy.batch_to_device\u001b[39m\u001b[34m(self, batch, device, dataloader_idx)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply_batch_transfer_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m move_data_to_device(batch, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/module.py:352\u001b[39m, in \u001b[36mLightningModule._apply_batch_transfer_handler\u001b[39m\u001b[34m(self, batch, device, dataloader_idx)\u001b[39m\n\u001b[32m    351\u001b[39m device = device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransfer_batch_to_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m batch = \u001b[38;5;28mself\u001b[39m._call_batch_hook(\u001b[33m\"\u001b[39m\u001b[33mon_after_batch_transfer\u001b[39m\u001b[33m\"\u001b[39m, batch, dataloader_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/module.py:341\u001b[39m, in \u001b[36mLightningModule._call_batch_hook\u001b[39m\u001b[34m(self, hook_name, *args)\u001b[39m\n\u001b[32m    339\u001b[39m         trainer_method = call._call_lightning_datamodule_hook\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m hook = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/core/hooks.py:611\u001b[39m, in \u001b[36mDataHooks.transfer_batch_to_device\u001b[39m\u001b[34m(self, batch, device, dataloader_idx)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[33;03mstructure.\u001b[39;00m\n\u001b[32m    567\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    609\u001b[39m \n\u001b[32m    610\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmove_data_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/fabric/utilities/apply_func.py:110\u001b[39m, in \u001b[36mmove_data_to_device\u001b[39m\u001b[34m(batch, device)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_TransferableDataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:74\u001b[39m, in \u001b[36mapply_to_collection\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# slow path for everything else\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_to_collection_slow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:127\u001b[39m, in \u001b[36m_apply_to_collection_slow\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     v = \u001b[43m_apply_to_collection_slow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m include_none \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:127\u001b[39m, in \u001b[36m_apply_to_collection_slow\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     v = \u001b[43m_apply_to_collection_slow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m include_none \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:98\u001b[39m, in \u001b[36m_apply_to_collection_slow\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m elem_type = \u001b[38;5;28mtype\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/fabric/utilities/apply_func.py:104\u001b[39m, in \u001b[36mmove_data_to_device.<locals>.batch_to\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mnon_blocking\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m data_output = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/chemprop/data/collate.py:68\u001b[39m, in \u001b[36mBatchMolGraph.to\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m | torch.device):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28mself\u001b[39m.V = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mV\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m.E = \u001b[38;5;28mself\u001b[39m.E.to(device)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      4\u001b[39m wandb_logger.experiment.mark_preempting()\n\u001b[32m      6\u001b[39m trainer = L.Trainer(\n\u001b[32m      7\u001b[39m     logger=wandb_logger,\n\u001b[32m      8\u001b[39m     enable_checkpointing=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     ]\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrastive_mpnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRandomPairDataModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_mol_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_mol_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:69\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[32m     71\u001b[39m     trainer.state.stage = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[39m, in \u001b[36mTrainer._teardown\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_teardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1033\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[33;03m    those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     loop = \u001b[38;5;28mself\u001b[39m._active_loop\n\u001b[32m   1037\u001b[39m     \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:536\u001b[39m, in \u001b[36mStrategy.teardown\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    535\u001b[39m     log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: moving model to CPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.precision_plugin.teardown()\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/lightning/fabric/utilities/device_dtype_mixin.py:82\u001b[39m, in \u001b[36m_DeviceDtypeModuleMixin.cpu\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m _update_properties(\u001b[38;5;28mself\u001b[39m, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1133\u001b[39m, in \u001b[36mModule.cpu\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) -> T:\n\u001b[32m   1125\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \n\u001b[32m   1127\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1131\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1132\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delta/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1133\u001b[39m, in \u001b[36mModule.cpu.<locals>.<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) -> T:\n\u001b[32m   1125\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \n\u001b[32m   1127\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1131\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1132\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"chemprop_delta_clf\", log_model=\"all\", save_code=True)\n",
    "wandb_logger.watch(contrastive_mpnn, log=\"gradients\", log_freq=50) \n",
    "wandb_logger.experiment.mark_preempting()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,  # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    # reload_dataloaders_every_n_epochs=1,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=10),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(contrastive_mpnn, datamodule=RandomPairDataModule(train_mol_dataset, val_mol_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run_id = wandb_logger.experiment.id\n",
    "checkpoint_reference = f\"rahul-e-dev/chemprop_delta_clf/model-{run_id}:best\"\n",
    "artifact_dir = wandb_logger.download_artifact(checkpoint_reference, artifact_type=\"model\")\n",
    "\n",
    "\n",
    "ckpt = torch.load(Path(artifact_dir) / \"model.ckpt\", map_location='cpu', weights_only=False)\n",
    "hparams = ckpt.get('hyper_parameters', ckpt.get('hparams', {}))\n",
    "contrastive_mpnn.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39709a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_mpnn = contrastive_mpnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701adf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_all(mol_dataset: data.datasets.MoleculeDataset, contrastive_mpnn):\n",
    "    dl = DataLoader(mol_dataset, batch_size=64, shuffle=False, collate_fn=data.dataloader.collate_batch)\n",
    "    all_embeds = []\n",
    "    for batch in tqdm(dl, total=len(dl)):\n",
    "        batch = move_data_to_device(batch, contrastive_mpnn.device)\n",
    "        res = contrastive_mpnn.embed_simple_batch(batch)\n",
    "        all_embeds.append(res['embeds'])\n",
    "\n",
    "    all_embeds = torch.cat(all_embeds)\n",
    "    return all_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cd777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca533926e3040218fcd644010f08e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded7887e3a2e495d8b234dbfed241fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_embeds = embed_all(train_mol_dataset, contrastive_mpnn)\n",
    "test_embeds = embed_all(test_mol_dataset, contrastive_mpnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplar_idxs = np.argwhere(train_mol_dataset.Y.squeeze() > 50)\n",
    "exemplar_embeds = train_embeds[exemplar_idxs].squeeze()\n",
    "exemplar_targets = train_mol_dataset.Y[exemplar_idxs].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_preds = contrastive_mpnn.interaction(exemplar_embeds, test_embeds)\n",
    "    all_preds = torch.exp(1 - torch.nn.functional.logsigmoid(preds).sum(axis=-1))\n",
    "    # all_preds = all_preds.mean(axis=-1)\n",
    "    all_preds = all_preds.detach().numpy().squeeze()\n",
    "    all_true = test_mol_dataset.Y > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08833c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, average_precision_score, roc_auc_score, balanced_accuracy_score, recall_score\n",
    "\n",
    "wandb_logger.log_table(\n",
    "    'final_metrics', \n",
    "    ['accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'AUCROC', 'PRAUC'],\n",
    "    [[\n",
    "        accuracy_score(all_true, all_preds > 0.5),\n",
    "        balanced_accuracy_score(all_true, all_preds > 0.5),\n",
    "        f1_score(all_true, all_preds > 0.5),\n",
    "        precision_score(all_true, all_preds > 0.5),\n",
    "        recall_score(all_true, all_preds > 0.5),\n",
    "        roc_auc_score(all_true, all_preds),\n",
    "        average_precision_score(all_true, all_preds)\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9727533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅█▅▅▅▇▇▆▆▄▆▅▆▄▅▄▃▄▆▅▆▅▄▃▄▅▃▃▅█▆▂▃▂▄▁▃▁</td></tr><tr><td>train_lr_exemplar_loss_epoch</td><td>█▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_lr_exemplar_loss_step</td><td>█▆▅▃▂▂▃▆▃▄▃▂▄▂▃▄▄▂▃▄▁▆▂▃▃▃▃▂▁▃▂▂▃▂▅▁▃▁▂▂</td></tr><tr><td>train_lr_random_loss_epoch</td><td>█▇▇▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train_lr_random_loss_step</td><td>█▆▅█▅▄█▅▅▆▅▄▄▅▄▆▅▄▁▅▄▅▄▄▂▃▃▄▃▃▂▂▁▆▂▃▁▂▃▃</td></tr><tr><td>train_rl_exemplar_loss_epoch</td><td>█▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_rl_exemplar_loss_step</td><td>█▅▄▄▄▃▄▃▃▃▃▂▃▆▂▃▂▂▃▃▂▄▂▃▃▅▃▂▂▁▁▃▃▁▃▁▂▂▂▂</td></tr><tr><td>train_rl_random_loss_epoch</td><td>█▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train_rl_random_loss_step</td><td>█▆▅▅▄▆▅▄▄▄▄▃▂▄▃▄▂▄▆▄▁▄▃▂▂▁▂▄▃▃▃▁▁▅▅▂▁▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▆█▆▅▃▃▃▂▅▂▂▂▂▁▂▁▂▁▁</td></tr><tr><td>val_lr_exemplar_loss</td><td>▆█▇▅▂▄▄▃▆▃▃▃▂▁▂▁▃▂▁</td></tr><tr><td>val_lr_random_loss</td><td>▅█▅▅▃▂▂▁▄▂▁▂▁▁▂▂▂▁▂</td></tr><tr><td>val_rl_exemplar_loss</td><td>▇█▇▆▄▄▄▃▆▃▃▃▂▂▃▁▃▂▁</td></tr><tr><td>val_rl_random_loss</td><td>▅█▄▅▃▂▃▁▃▂▁▂▁▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>train_loss_epoch</td><td>0.32553</td></tr><tr><td>train_loss_step</td><td>0.27398</td></tr><tr><td>train_lr_exemplar_loss_epoch</td><td>0.17796</td></tr><tr><td>train_lr_exemplar_loss_step</td><td>0.13512</td></tr><tr><td>train_lr_random_loss_epoch</td><td>0.47117</td></tr><tr><td>train_lr_random_loss_step</td><td>0.41949</td></tr><tr><td>train_rl_exemplar_loss_epoch</td><td>0.18429</td></tr><tr><td>train_rl_exemplar_loss_step</td><td>0.14008</td></tr><tr><td>train_rl_random_loss_epoch</td><td>0.4687</td></tr><tr><td>train_rl_random_loss_step</td><td>0.40124</td></tr><tr><td>trainer/global_step</td><td>6155</td></tr><tr><td>val_loss</td><td>0.52014</td></tr><tr><td>val_lr_exemplar_loss</td><td>0.42134</td></tr><tr><td>val_lr_random_loss</td><td>0.61317</td></tr><tr><td>val_rl_exemplar_loss</td><td>0.43356</td></tr><tr><td>val_rl_random_loss</td><td>0.6125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-wave-211</strong> at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/jsdt8615' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/jsdt8615</a><br> View project at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a><br>Synced 6 W&B file(s), 1 media file(s), 32 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_231644-jsdt8615/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99326675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad280da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_all(mol_dataset: data.datasets.MoleculeDataset, contrastive_mpnn):\n",
    "    dl = DataLoader(mol_dataset, batch_size=64, shuffle=False, collate_fn=data.dataloader.collate_batch)\n",
    "    all_embeds = []\n",
    "    for batch in tqdm(dl, total=len(dl)):\n",
    "        batch = move_data_to_device(batch, contrastive_mpnn.device)\n",
    "        res = contrastive_mpnn.embed_simple_batch(batch)\n",
    "        all_embeds.append(res['embeds'])\n",
    "\n",
    "    all_embeds = torch.cat(all_embeds)\n",
    "    return all_embeds\n",
    "\n",
    "@torch.no_grad()\n",
    "def mine_hard_and_rand_negatives_for_anchor(anchor_idx, all_embeds: torch.Tensor, all_targets: torch.Tensor, clf: torch.nn.Linear):\n",
    "    anchor = all_embeds[anchor_idx]\n",
    "    anchor = anchor.view(1, -1).expand_as(all_embeds)\n",
    "    logits = clf(torch.cat([anchor, all_embeds], dim=-1)).squeeze()\n",
    "    anchor_target = all_targets[anchor_idx]\n",
    "\n",
    "    negative_mask = torch.logical_xor(logits > 0.5, anchor_target > all_targets)\n",
    "    logits[~negative_mask] = float(\"-inf\")\n",
    "\n",
    "    _, hard_neg_idxs = torch.topk(logits, 3, largest=True)\n",
    "\n",
    "    negative_mask[hard_neg_idxs] = False\n",
    "\n",
    "    rand_neg_slection_prob = torch.where(negative_mask, 1.0, float(\"-inf\")).softmax(dim=-1)\n",
    "    rand_neg_idxs = torch.multinomial(rand_neg_slection_prob, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b8dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bb30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
