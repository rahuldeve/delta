{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763a621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/rahul/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul-e-dev\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data, featurizers, models\n",
    "from chemprop import nn as chem_nn\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "import pandas as pd\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from commons.utils import get_scaffold, standardize\n",
    "from typing import NamedTuple\n",
    "from itertools import chain\n",
    "\n",
    "import wandb\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "wandb.login(key=\"cf344975eb80edf6f0d52af80528cc6094234caf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb18a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_inchi(mol):\n",
    "    with BlockLogs():\n",
    "        return Chem.MolToInchi(mol)\n",
    "\n",
    "\n",
    "def load_and_split_gsk_dataset(path, RANDOM_SEED):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[:, 1:]\n",
    "    df.columns = [\"smiles\", \"per_inhibition\"]\n",
    "\n",
    "    # standardize and convert to inchi\n",
    "    df[\"mol\"] = df[\"smiles\"].map(standardize)\n",
    "    df = df.dropna(subset=[\"mol\"])\n",
    "    df[\"inchi\"] = df[\"mol\"].map(mol_to_inchi)\n",
    "    df = df.groupby([\"inchi\"]).filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "\n",
    "    clusters, _ = pd.factorize(\n",
    "        df[\"mol\"]\n",
    "        .map(Chem.MolToSmiles)  # type: ignore\n",
    "        .map(get_scaffold)\n",
    "    )\n",
    "    clusters = pd.Series(clusters)\n",
    "\n",
    "    df = df.drop([\"smiles\", \"inchi\"], axis=1)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED)\n",
    "    train_idxs, val_test_idxs = next(splitter.split(df, groups=clusters))\n",
    "    df_train = df.loc[train_idxs].reset_index(drop=True)\n",
    "    df_val_test = df.loc[val_test_idxs].reset_index(drop=True)\n",
    "    clusters_val_test = clusters.iloc[val_test_idxs].reset_index(drop=True)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, random_state=RANDOM_SEED, test_size=0.5)\n",
    "    val_idxs, test_idxs = next(splitter.split(df_val_test, groups=clusters_val_test))\n",
    "    df_val = df_val_test.loc[val_idxs].reset_index(drop=True)\n",
    "    df_test = df_val_test.loc[test_idxs].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0aecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_and_split_gsk_dataset(\"../GSK_HepG2.csv\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1974a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_molecule_datapoint(x):\n",
    "    return data.MoleculeDatapoint(x['mol'], x['per_inhibition'])\n",
    "\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "train_mol_dataset = data.MoleculeDataset(df_train.apply(mol_to_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "val_mol_dataset = data.MoleculeDataset(df_val.apply(mol_to_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "test_mol_dataset = data.MoleculeDataset(df_test.apply(mol_to_molecule_datapoint, axis=1), featurizer=featurizer)\n",
    "\n",
    "train_mol_dataset.cache = True\n",
    "val_mol_dataset.cache = True\n",
    "test_mol_dataset.cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa43d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPairDataPoint(NamedTuple):\n",
    "    anchor: data.datasets.Datum\n",
    "    candidates: list[data.datasets.Datum]\n",
    "\n",
    "\n",
    "class RandomPairTrainBatch(NamedTuple):\n",
    "    anchor: data.collate.TrainingBatch\n",
    "    candidates: data.collate.TrainingBatch\n",
    "    B: int\n",
    "    C: int\n",
    "    \n",
    "\n",
    "class RandomPairDataset(Dataset):\n",
    "    def __init__(self, mol_dataset, candidate_size):\n",
    "        super().__init__()\n",
    "        self.mol_dataset: data.datasets.MoleculeDataset = mol_dataset\n",
    "        self.candidate_size: int = candidate_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mol_dataset)\n",
    "    \n",
    "    def get_random_candidates(self):\n",
    "        targets = self.mol_dataset.Y.squeeze()\n",
    "        mask = targets > 50\n",
    "        probs = np.where(mask, 4.0, 1.0)\n",
    "        probs = probs / probs.sum()\n",
    "        candidate_idxs = np.random.choice(targets.shape[0], size=(self.candidate_size,), p=probs, replace=False)\n",
    "        return [self.mol_dataset[idx] for idx in candidate_idxs]\n",
    "\n",
    "    def __getitem__(self, idx) -> RandomPairDataPoint:\n",
    "        return RandomPairDataPoint(\n",
    "            self.mol_dataset[idx], \n",
    "            self.get_random_candidates()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_function(batch):\n",
    "        batch_anchors, batch_candidates = zip(*batch)\n",
    "        B = len(batch)\n",
    "        C = len(batch_candidates[0])\n",
    "        batch_anchors = data.dataloader.collate_batch(batch_anchors)\n",
    "        batch_candidates = data.dataloader.collate_batch(chain.from_iterable(batch_candidates))\n",
    "        return RandomPairTrainBatch(batch_anchors, batch_candidates, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe77114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class RandomPairDataModule(L.LightningDataModule):\n",
    "    def __init__(self, mol_ds_train, mol_ds_val) -> None:\n",
    "        super().__init__()\n",
    "        self.mol_ds_train: data.MoleculeDataset = mol_ds_train\n",
    "        self.mol_ds_val: data.MoleculeDataset = mol_ds_val\n",
    "        self.batch_size=16\n",
    "        self.candidate_size=64\n",
    "\n",
    "        self.ds_train = None\n",
    "        self.ds_val = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds_train = RandomPairDataset(self.mol_ds_train, self.candidate_size)\n",
    "        self.ds_val = RandomPairDataset(self.mol_ds_val, self.candidate_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        assert self.ds_train is not None\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=RandomPairDataset.collate_function,\n",
    "            worker_init_fn=seed_worker,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        assert self.ds_val is not None\n",
    "        return DataLoader(\n",
    "            self.ds_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=RandomPairDataset.collate_function,\n",
    "            num_workers=8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22de2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from chemprop.nn import Aggregation, ChempropMetric, MessagePassing, Predictor\n",
    "from chemprop.nn.transforms import ScaleTransform\n",
    "\n",
    "\n",
    "class ContrastiveMPNN(models.MPNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: MessagePassing,\n",
    "        agg: Aggregation,\n",
    "        predictor: Predictor,\n",
    "        batch_norm: bool = False,\n",
    "        metrics: Iterable[ChempropMetric] | None = None,\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 0.0001,\n",
    "        max_lr: float = 0.001,\n",
    "        final_lr: float = 0.0001,\n",
    "        X_d_transform: ScaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message_passing,\n",
    "            agg,\n",
    "            predictor,\n",
    "            batch_norm,\n",
    "            metrics,\n",
    "            warmup_epochs,\n",
    "            init_lr,\n",
    "            max_lr,\n",
    "            final_lr,\n",
    "            X_d_transform,\n",
    "        )\n",
    "        \n",
    "        self.clf = torch.nn.Sequential(\n",
    "            torch.nn.Linear(600, 300),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(300, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "    def embed_simple_batch(self, batch: data.collate.TrainingBatch):\n",
    "        bmg, V_d, X_d, target, _, _, _ = batch\n",
    "        Z = self.encoding(bmg, V_d, X_d)\n",
    "        return dict(embeds=Z, targets=target)\n",
    "    \n",
    "\n",
    "    def get_loss(self, batch: RandomPairTrainBatch, stage: str):\n",
    "        B, C = batch.B, batch.C\n",
    "\n",
    "        bmg, V_d, X_d, target_anchor, _, _, _ = batch.anchor\n",
    "        Z_anchor = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        bmg, V_d, X_d, target_candidates, _, _, _ = batch.candidates\n",
    "        Z_candidates = self.encoding(bmg, V_d, X_d)\n",
    "\n",
    "        # # self loss\n",
    "        # Z_combined = torch.cat([Z_anchor, Z_anchor], dim=-1)\n",
    "        # preds = self.clf(Z_combined).squeeze()\n",
    "        # self_loss = self.loss_fn(preds, torch.zeros_like(preds))\n",
    "        # self.log(f\"{stage}_self_loss\", self_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "\n",
    "        Z_anchor = Z_anchor.view(B, 1, -1).expand(B, C, -1)     # (B, d) -> (B, 1, d) -> (B, C, d)\n",
    "        Z_candidates = Z_candidates.view(B, C, -1)              # (B*C, d) -> (B, C, d)\n",
    "\n",
    "        # left to right loss\n",
    "        Z_combined = torch.cat([Z_anchor, Z_candidates], dim=-1)    # (B, C, 2*d)\n",
    "        preds = self.clf(Z_combined).squeeze()\n",
    "        labels = (target_anchor.view(-1, 1) > target_candidates.view(B, C)).float() # type: ignore\n",
    "        lr_batch_loss = self.loss_fn(preds, labels)\n",
    "        self.log(f\"{stage}_lr_batch_loss\", lr_batch_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "\n",
    "        # right to left loss\n",
    "        Z_combined = torch.cat([Z_candidates, Z_anchor], dim=-1)    # (B, C, 2*d)\n",
    "        preds = self.clf(Z_combined).squeeze()\n",
    "        labels = (target_candidates.view(B, C) > target_anchor.view(-1, 1)).float() # type: ignore\n",
    "        rl_batch_loss = self.loss_fn(preds, labels)\n",
    "        self.log(f\"{stage}_rl_batch_loss\", rl_batch_loss, batch_size=batch.B, on_epoch=True, enable_graph=True)\n",
    "\n",
    "\n",
    "        return lr_batch_loss + rl_batch_loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch: RandomPairTrainBatch, batch_idx):  # type: ignore\n",
    "        loss = self.get_loss(batch, 'train')\n",
    "        self.log(\"train_loss\", loss, batch_size=batch.B, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: RandomPairTrainBatch, batch_idx):  # type: ignore\n",
    "        loss = self.get_loss(batch, 'val')\n",
    "        self.log(\"val_loss\", loss, batch_size=batch.B)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8594915",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdims = featurizers.SimpleMoleculeMolGraphFeaturizer().shape # the dimensions of the featurizer, given as (atom_dims, bond_dims).\n",
    "mp = chem_nn.BondMessagePassing()\n",
    "agg = chem_nn.NormAggregation()\n",
    "ffn = chem_nn.BinaryClassificationFFN(n_tasks=1)\n",
    "batch_norm = True\n",
    "metric_list = [chem_nn.metrics.BinaryF1Score(), chem_nn.metrics.BinaryAUPRC(), chem_nn.metrics.BinaryAUROC()]\n",
    "contrastive_mpnn = ContrastiveMPNN(mp, agg, ffn, batch_norm, metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e0455da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20251011_212027-nz2ie9b3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/nz2ie9b3' target=\"_blank\">clean-armadillo-158</a></strong> to <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/nz2ie9b3' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/nz2ie9b3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type                    | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing      | 227 K  | train\n",
      "1 | agg             | NormAggregation         | 0      | train\n",
      "2 | bn              | BatchNorm1d             | 600    | train\n",
      "3 | predictor       | BinaryClassificationFFN | 90.6 K | train\n",
      "4 | X_d_transform   | Identity                | 0      | train\n",
      "5 | metrics         | ModuleList              | 0      | train\n",
      "6 | clf             | Sequential              | 180 K  | train\n",
      "7 | loss_fn         | BCELoss                 | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "499 K     Trainable params\n",
      "0         Non-trainable params\n",
      "499 K     Total params\n",
      "1.998     Total estimated model params size (MB)\n",
      "32        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247f27d45fb34f279d5ee308dae1fca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e84d8f3cd8a4bf68b4904803e0c83cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea11ab0f7a64143a1fbd460b8f545a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98b94dad33e4ae383f407f368e0f1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.174 >= min_delta = 0.0. New best score: 1.089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfba4a04ba34d5cb1276c203cfeeae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72886e6c00c4fe3a3c59f481b7eaa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 1.076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5302288ecf444c66a54dcaff0c480f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bdc1befe344e4193fc228953f5fbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3fc534c5644d61b089025b916a7333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24231fbff8c4277b65121c186c3e034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60fd0aac7884b6aa380863df159174c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6517b5748832434ea0c73a87155fb944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed3d6f44134494692e5b5f012b57d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a22e6bcb0b44dc09c55f236e483503f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31041c4320e946b9b4deee916f6c8e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647118bdd2c74b1aaee61d4e63612510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 1.076. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"chemprop_delta_clf\", log_model=\"all\", save_code=True)\n",
    "wandb_logger.experiment.mark_preempting()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,  # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=50,  # number of epochs to train for\n",
    "    # reload_dataloaders_every_n_epochs=1,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True, patience=10),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(contrastive_mpnn, datamodule=RandomPairDataModule(train_mol_dataset, val_mol_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41a213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run_id = wandb_logger.experiment.id\n",
    "checkpoint_reference = f\"rahul-e-dev/chemprop_delta_clf/model-{run_id}:best\"\n",
    "artifact_dir = wandb_logger.download_artifact(checkpoint_reference, artifact_type=\"model\")\n",
    "\n",
    "\n",
    "ckpt = torch.load(Path(artifact_dir) / \"model.ckpt\", map_location='cpu', weights_only=False)\n",
    "hparams = ckpt.get('hyper_parameters', ckpt.get('hparams', {}))\n",
    "contrastive_mpnn.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "# trainer = L.Trainer(\n",
    "#     enable_progress_bar=True,\n",
    "#     accelerator=\"auto\",\n",
    "#     devices=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39709a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_mpnn = contrastive_mpnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "701adf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_all(mol_dataset: data.datasets.MoleculeDataset, contrastive_mpnn):\n",
    "    dl = DataLoader(mol_dataset, batch_size=64, shuffle=False, collate_fn=data.dataloader.collate_batch)\n",
    "    all_embeds = []\n",
    "    for batch in tqdm(dl, total=len(dl)):\n",
    "        batch = move_data_to_device(batch, contrastive_mpnn.device)\n",
    "        res = contrastive_mpnn.embed_simple_batch(batch)\n",
    "        all_embeds.append(res['embeds'])\n",
    "\n",
    "    all_embeds = torch.cat(all_embeds)\n",
    "    return all_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53cd777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a8a6547457435f9bd588c7c6334f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09be699025054217bbbd4d9215a6a683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_embeds = embed_all(train_mol_dataset, contrastive_mpnn)\n",
    "test_embeds = embed_all(test_mol_dataset, contrastive_mpnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee1ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplar_idxs = np.argwhere(train_mol_dataset.Y > 50)\n",
    "exemplar_embeds = train_embeds[exemplar_idxs].squeeze()\n",
    "exemplar_targets = train_mol_dataset.Y[exemplar_idxs].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70f25a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(test_embeds.shape[0]):\n",
    "        Z_anchor = test_embeds[idx].view(1, -1).expand_as(exemplar_embeds)\n",
    "        comb = torch.cat([Z_anchor, exemplar_embeds], dim=-1)\n",
    "        preds = contrastive_mpnn.clf(comb).mean()\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_true.append(test_mol_dataset.Y[idx] > 50)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_true = np.array(all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bcb90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, average_precision_score, roc_auc_score, balanced_accuracy_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac8cd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.log_table(\n",
    "    'final_metrics', \n",
    "    ['accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'AUCROC', 'PRAUC'],\n",
    "    [[\n",
    "        accuracy_score(all_true, all_preds > 0.5),\n",
    "        balanced_accuracy_score(all_true, all_preds > 0.5),\n",
    "        f1_score(all_true, all_preds > 0.5),\n",
    "        precision_score(all_true, all_preds > 0.5),\n",
    "        recall_score(all_true, all_preds > 0.5),\n",
    "        roc_auc_score(all_true, all_preds),\n",
    "        average_precision_score(all_true, all_preds)\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9727533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▄▄▃▃▂▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▆▅▄▄▄▆▄▄▄▄▄▃▃▄▄▄▃▃▃▄▂▂▃▃▃▃▃▂▂▂▂▂▁▁▃▂▂</td></tr><tr><td>train_lr_batch_loss_epoch</td><td>█▇▆▅▄▄▃▃▂▂▂▂▁▁</td></tr><tr><td>train_lr_batch_loss_step</td><td>█▆▇▆▆▅▆▄▃▄▄▄▂▄▄▄▃▄▄▄▄▂▂▂▄▂▃▃▂▃▂▁▂▂▂▁▂▃▂▂</td></tr><tr><td>train_rl_batch_loss_epoch</td><td>█▇▆▅▄▄▃▃▂▂▂▂▁▁</td></tr><tr><td>train_rl_batch_loss_step</td><td>▇██▇▆▇▅▅▆▆▇▅▄▅▅▆▃▅▄▄▄▅▅▅▃▂▄▂▂▃▃▃▁▃▁▁▃▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇█████</td></tr><tr><td>val_loss</td><td>▅▁▂▁▂▂▄▂▄▄▅▅▇█</td></tr><tr><td>val_lr_batch_loss</td><td>▅▁▂▁▂▂▄▂▄▄▅▅▇█</td></tr><tr><td>val_rl_batch_loss</td><td>▅▁▂▁▂▂▄▂▄▄▅▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_loss_epoch</td><td>0.56043</td></tr><tr><td>train_loss_step</td><td>0.5973</td></tr><tr><td>train_lr_batch_loss_epoch</td><td>0.27981</td></tr><tr><td>train_lr_batch_loss_step</td><td>0.29264</td></tr><tr><td>train_rl_batch_loss_epoch</td><td>0.28062</td></tr><tr><td>train_rl_batch_loss_step</td><td>0.30465</td></tr><tr><td>trainer/global_step</td><td>9071</td></tr><tr><td>val_loss</td><td>1.3719</td></tr><tr><td>val_lr_batch_loss</td><td>0.68368</td></tr><tr><td>val_rl_batch_loss</td><td>0.68821</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-armadillo-158</strong> at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/nz2ie9b3' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf/runs/nz2ie9b3</a><br> View project at: <a href='https://wandb.ai/rahul-e-dev/chemprop_delta_clf' target=\"_blank\">https://wandb.ai/rahul-e-dev/chemprop_delta_clf</a><br>Synced 6 W&B file(s), 1 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251011_212027-nz2ie9b3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99326675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad280da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_all(mol_dataset: data.datasets.MoleculeDataset, contrastive_mpnn):\n",
    "    dl = DataLoader(mol_dataset, batch_size=64, shuffle=False, collate_fn=data.dataloader.collate_batch)\n",
    "    all_embeds = []\n",
    "    for batch in tqdm(dl, total=len(dl)):\n",
    "        batch = move_data_to_device(batch, contrastive_mpnn.device)\n",
    "        res = contrastive_mpnn.embed_simple_batch(batch)\n",
    "        all_embeds.append(res['embeds'])\n",
    "\n",
    "    all_embeds = torch.cat(all_embeds)\n",
    "    return all_embeds\n",
    "\n",
    "@torch.no_grad()\n",
    "def mine_hard_and_rand_negatives_for_anchor(anchor_idx, all_embeds: torch.Tensor, all_targets: torch.Tensor, clf: torch.nn.Linear):\n",
    "    anchor = all_embeds[anchor_idx]\n",
    "    anchor = anchor.view(1, -1).expand_as(all_embeds)\n",
    "    logits = clf(torch.cat([anchor, all_embeds], dim=-1)).squeeze()\n",
    "    anchor_target = all_targets[anchor_idx]\n",
    "\n",
    "    negative_mask = torch.logical_xor(logits > 0.5, anchor_target > all_targets)\n",
    "    logits[~negative_mask] = float(\"-inf\")\n",
    "\n",
    "    _, hard_neg_idxs = torch.topk(logits, 3, largest=True)\n",
    "\n",
    "    negative_mask[hard_neg_idxs] = False\n",
    "\n",
    "    rand_neg_slection_prob = torch.where(negative_mask, 1.0, float(\"-inf\")).softmax(dim=-1)\n",
    "    rand_neg_idxs = torch.multinomial(rand_neg_slection_prob, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b8dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bb30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
